{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wfHG1DLNBNu",
        "outputId": "fbc6c613-6033-4ed3-f451-9b39abd04bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m933.5/933.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.3/383.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.4 which is incompatible.\n",
            "notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q jupyter_ai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "load_dotenv(\"/content/drive/MyDrive/Udemy/LangChain/p-1/.env\", override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI3wZe6eNS_2",
        "outputId": "cdcb5147-6189-43ff-9dc4-6628177d35a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext jupyter_ai_magics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ8kySJyN2tX",
        "outputId": "c18b9551-6989-4685-9e3f-afe44c03a212"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Unable to load model provider `anthropic`. Please install the `langchain_anthropic` package.\n",
            "WARNING:root:Unable to load model provider `anthropic-chat`. Please install the `langchain_anthropic` package.\n",
            "WARNING:root:Unable to load model provider `azure-chat-openai`. Please install the `langchain_openai` package.\n",
            "WARNING:root:Unable to load model provider `gemini`. Please install the `langchain_google_genai` package.\n",
            "WARNING:root:Unable to load model provider `nvidia-chat`. Please install the `langchain_nvidia_ai_endpoints` package.\n",
            "WARNING:root:Unable to load model provider `openai`. Please install the `langchain_openai` package.\n",
            "WARNING:root:Unable to load model provider `openai-chat`. Please install the `langchain_openai` package.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ai help"
      ],
      "metadata": {
        "id": "F_aTOZPfOMcS",
        "outputId": "bd35e082-6d88-48d7-f77d-009fe7843378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: %%ai [OPTIONS] MODEL_ID\n",
            "\n",
            "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
            "  contained in all lines after the first. Both local model IDs and global\n",
            "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
            "  are accepted.\n",
            "\n",
            "  To view available language models, please run `%ai list`.\n",
            "\n",
            "Options:\n",
            "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
            "                                  IPython display to use when rendering\n",
            "                                  output. [default=\"markdown\"]\n",
            "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
            "                                  for SageMaker provider; does nothing with\n",
            "                                  other providers.\n",
            "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
            "                                  the prompt being substituted into any value\n",
            "                                  that matches the string literal '<prompt>'.\n",
            "                                  Required for SageMaker provider; does\n",
            "                                  nothing with other providers.\n",
            "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
            "                                  language model's output from the endpoint's\n",
            "                                  JSON response. Required for SageMaker\n",
            "                                  provider; does nothing with other providers.\n",
            "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
            "                                  that will be passed to the model. The\n",
            "                                  accepted value parsed to a dict, unpacked\n",
            "                                  and passed as-is to the provider class.\n",
            "  --help                          Show this message and exit.\n",
            "------------------------------------------------------------------------------\n",
            "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
            "\n",
            "  Invokes a subcommand.\n",
            "\n",
            "Options:\n",
            "  --help  Show this message and exit.\n",
            "\n",
            "Commands:\n",
            "  delete    Delete an alias. See `%ai delete --help` for options.\n",
            "  error     Explains the most recent error.\n",
            "  help      Show this message and exit.\n",
            "  list      List language models. See `%ai list --help` for options.\n",
            "  register  Register a new alias. See `%ai register --help` for options.\n",
            "  update    Update the target of an alias. See `%ai update --help` for\n",
            "            options.\n",
            "  version   Prints Jupyter-AI version\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ai list --help"
      ],
      "metadata": {
        "id": "tyKKmi7aOSVB",
        "outputId": "fa6a4208-1f53-4256-dba1-ab8cf47b4266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: %ai list [OPTIONS] [PROVIDER_ID]\n",
            "\n",
            "  List language models, optionally scoped to PROVIDER_ID.\n",
            "\n",
            "Options:\n",
            "  --help  Show this message and exit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ai help"
      ],
      "metadata": {
        "id": "sbBhJzoLObER",
        "outputId": "160000d8-ce4d-4317-feb8-59095972f326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: %%ai [OPTIONS] MODEL_ID\n",
            "\n",
            "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
            "  contained in all lines after the first. Both local model IDs and global\n",
            "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
            "  are accepted.\n",
            "\n",
            "  To view available language models, please run `%ai list`.\n",
            "\n",
            "Options:\n",
            "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
            "                                  IPython display to use when rendering\n",
            "                                  output. [default=\"markdown\"]\n",
            "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
            "                                  for SageMaker provider; does nothing with\n",
            "                                  other providers.\n",
            "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
            "                                  the prompt being substituted into any value\n",
            "                                  that matches the string literal '<prompt>'.\n",
            "                                  Required for SageMaker provider; does\n",
            "                                  nothing with other providers.\n",
            "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
            "                                  language model's output from the endpoint's\n",
            "                                  JSON response. Required for SageMaker\n",
            "                                  provider; does nothing with other providers.\n",
            "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
            "                                  that will be passed to the model. The\n",
            "                                  accepted value parsed to a dict, unpacked\n",
            "                                  and passed as-is to the provider class.\n",
            "  --help                          Show this message and exit.\n",
            "------------------------------------------------------------------------------\n",
            "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
            "\n",
            "  Invokes a subcommand.\n",
            "\n",
            "Options:\n",
            "  --help  Show this message and exit.\n",
            "\n",
            "Commands:\n",
            "  delete    Delete an alias. See `%ai delete --help` for options.\n",
            "  error     Explains the most recent error.\n",
            "  help      Show this message and exit.\n",
            "  list      List language models. See `%ai list --help` for options.\n",
            "  register  Register a new alias. See `%ai register --help` for options.\n",
            "  update    Update the target of an alias. See `%ai update --help` for\n",
            "            options.\n",
            "  version   Prints Jupyter-AI version\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ai list"
      ],
      "metadata": {
        "id": "Go8eLvfzSQWd",
        "outputId": "11857038-c32f-4fbf-d634-5c7bef62be6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ai21\n",
              "Requires environment variable: AI21_API_KEY (not set)\n",
              "* ai21:j1-large\n",
              "* ai21:j1-grande\n",
              "* ai21:j1-jumbo\n",
              "* ai21:j1-grande-instruct\n",
              "* ai21:j2-large\n",
              "* ai21:j2-grande\n",
              "* ai21:j2-jumbo\n",
              "* ai21:j2-grande-instruct\n",
              "* ai21:j2-jumbo-instruct\n",
              "\n",
              "bedrock\n",
              "* bedrock:amazon.titan-text-express-v1\n",
              "* bedrock:amazon.titan-text-lite-v1\n",
              "* bedrock:ai21.j2-ultra-v1\n",
              "* bedrock:ai21.j2-mid-v1\n",
              "* bedrock:cohere.command-light-text-v14\n",
              "* bedrock:cohere.command-text-v14\n",
              "* bedrock:cohere.command-r-v1:0\n",
              "* bedrock:cohere.command-r-plus-v1:0\n",
              "* bedrock:meta.llama2-13b-chat-v1\n",
              "* bedrock:meta.llama2-70b-chat-v1\n",
              "* bedrock:meta.llama3-8b-instruct-v1:0\n",
              "* bedrock:meta.llama3-70b-instruct-v1:0\n",
              "* bedrock:mistral.mistral-7b-instruct-v0:2\n",
              "* bedrock:mistral.mixtral-8x7b-instruct-v0:1\n",
              "* bedrock:mistral.mistral-large-2402-v1:0\n",
              "\n",
              "bedrock-chat\n",
              "* bedrock-chat:anthropic.claude-v2\n",
              "* bedrock-chat:anthropic.claude-v2:1\n",
              "* bedrock-chat:anthropic.claude-instant-v1\n",
              "* bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0\n",
              "* bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0\n",
              "* bedrock-chat:anthropic.claude-3-opus-20240229-v1:0\n",
              "\n",
              "cohere\n",
              "Requires environment variable: COHERE_API_KEY (not set)\n",
              "* cohere:command\n",
              "* cohere:command-nightly\n",
              "* cohere:command-light\n",
              "* cohere:command-light-nightly\n",
              "\n",
              "gpt4all\n",
              "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
              "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
              "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
              "* gpt4all:mistral-7b-openorca.Q4_0\n",
              "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
              "* gpt4all:gpt4all-falcon-q4_0\n",
              "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
              "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
              "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
              "* gpt4all:mpt-7b-chat-merges-q4_0\n",
              "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
              "* gpt4all:starcoder-q4_0\n",
              "* gpt4all:rift-coder-v0-7b-q4_0\n",
              "* gpt4all:em_german_mistral_v01.Q4_0\n",
              "\n",
              "huggingface_hub\n",
              "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (not set)\n",
              "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
              "\n",
              "qianfan\n",
              "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
              "* qianfan:ERNIE-Bot\n",
              "* qianfan:ERNIE-Bot-4\n",
              "\n",
              "sagemaker-endpoint\n",
              "* Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
              "\n",
              "togetherai\n",
              "Requires environment variable: TOGETHER_API_KEY (not set)\n",
              "* togetherai:Austism/chronos-hermes-13b\n",
              "* togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2\n",
              "* togetherai:EleutherAI/llemma_7b\n",
              "* togetherai:Gryphe/MythoMax-L2-13b\n",
              "* togetherai:Meta-Llama/Llama-Guard-7b\n",
              "* togetherai:Nexusflow/NexusRaven-V2-13B\n",
              "* togetherai:NousResearch/Nous-Capybara-7B-V1p9\n",
              "* togetherai:NousResearch/Nous-Hermes-2-Yi-34B\n",
              "* togetherai:NousResearch/Nous-Hermes-Llama2-13b\n",
              "* togetherai:NousResearch/Nous-Hermes-Llama2-70b\n",
              "\n",
              "\n",
              "Aliases and custom commands:\n",
              "gpt2 - huggingface_hub:gpt2\n",
              "gpt3 - openai:davinci-002\n",
              "chatgpt - openai-chat:gpt-3.5-turbo\n",
              "gpt4 - openai-chat:gpt-4\n",
              "ernie-bot - qianfan:ERNIE-Bot\n",
              "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
              "titan - bedrock:amazon.titan-tg1-large\n"
            ],
            "text/markdown": "| Provider | Environment variable | Set? | Models |\n|----------|----------------------|------|--------|\n| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock:amazon.titan-text-express-v1`</li><li>`bedrock:amazon.titan-text-lite-v1`</li><li>`bedrock:ai21.j2-ultra-v1`</li><li>`bedrock:ai21.j2-mid-v1`</li><li>`bedrock:cohere.command-light-text-v14`</li><li>`bedrock:cohere.command-text-v14`</li><li>`bedrock:cohere.command-r-v1:0`</li><li>`bedrock:cohere.command-r-plus-v1:0`</li><li>`bedrock:meta.llama2-13b-chat-v1`</li><li>`bedrock:meta.llama2-70b-chat-v1`</li><li>`bedrock:meta.llama3-8b-instruct-v1:0`</li><li>`bedrock:meta.llama3-70b-instruct-v1:0`</li><li>`bedrock:mistral.mistral-7b-instruct-v0:2`</li><li>`bedrock:mistral.mixtral-8x7b-instruct-v0:1`</li><li>`bedrock:mistral.mistral-large-2402-v1:0`</li></ul> |\n| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock-chat:anthropic.claude-v2`</li><li>`bedrock-chat:anthropic.claude-v2:1`</li><li>`bedrock-chat:anthropic.claude-instant-v1`</li><li>`bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-opus-20240229-v1:0`</li></ul> |\n| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`cohere:command`</li><li>`cohere:command-nightly`</li><li>`cohere:command-light`</li><li>`cohere:command-light-nightly`</li></ul> |\n| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n\nAliases and custom commands:\n\n| Name | Target |\n|------|--------|\n| `gpt2` | `huggingface_hub:gpt2` |\n| `gpt3` | `openai:davinci-002` |\n| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n| `gpt4` | `openai-chat:gpt-4` |\n| `ernie-bot` | `qianfan:ERNIE-Bot` |\n| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n| `titan` | `bedrock:amazon.titan-tg1-large` |\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ai list openai"
      ],
      "metadata": {
        "id": "KP0WixaHSZUS",
        "outputId": "88031a5e-55c2-48fe-95e1-0f626880f6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "There is no model provider with ID 'openai'."
            ],
            "text/markdown": "There is no model provider with ID `openai`."
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ai openai-chat:gpt-3.5-turbo\n",
        "list vs. tuple vs. set in python"
      ],
      "metadata": {
        "id": "9uz05gWuUymQ",
        "outputId": "69c758d9-32b5-4830-b65a-f9f7abf0232e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-9-f1a724753452>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-f1a724753452>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    list vs. tuple vs. set in python\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ai chatgpt\n",
        "How to sort list in python?"
      ],
      "metadata": {
        "id": "PsvAEl8eV8hj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}