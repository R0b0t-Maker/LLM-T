{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaVCcfMkhsjoedbI4bg46B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R0b0t-Maker/LLM-T/blob/main/Chatbot_LangChain_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-google-genai"
      ],
      "metadata": {
        "id": "yHq4UeKJV__J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q langchain-google-genai"
      ],
      "metadata": {
        "id": "-YeH6UvTXBC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvY6sDu7XM9B",
        "outputId": "b416143e-ede7-41ae-b934-6f724e20a7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain-google-genai\n",
            "Version: 1.0.5\n",
            "Summary: An integration package connecting Google's genai package and LangChain\n",
            "Home-page: https://github.com/langchain-ai/langchain-google\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: google-generativeai, langchain-core\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai"
      ],
      "metadata": {
        "id": "8GQGQkKLXf-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "i19ookyoYm0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjZqEuIVY4g6",
        "outputId": "ef3126d3-a2c7-4bb3-b3b8-f9967616eff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(\"/content/drive/MyDrive/Udemy/LangChain/p-1/.env\", override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8RHMkFKZJ_S",
        "outputId": "ff3f67f3-4a04-421e-c395-3264f78f9556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if 'GOOGLE_API_KEY' not in os.environ:\n",
        "  os.environ['GOOGLE_APIKEY']= getpass.getpass('Insert the Google API key:')\n"
      ],
      "metadata": {
        "id": "HY6dncuZzxAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "for model in genai.list_models():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "id": "m761CTH60sFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pathlib\n",
        "# import textwrap\n",
        "\n",
        "# import google.generativeai as genai\n",
        "\n",
        "# from IPython.display import display\n",
        "# from IPython.display import Markdown\n",
        "\n",
        "\n",
        "# def to_markdown(text):\n",
        "#   text = text.replace('•', '  *')\n",
        "#   return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "# from google.colab import userdata\n",
        "\n",
        "\n",
        "# GOOGLE_API_KEY=userdata.get('-----------------')\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "\n",
        "# for m in genai.list_models():\n",
        "#   if 'generateContent' in m.supported_generation_methods:\n",
        "#     print(m.name)"
      ],
      "metadata": {
        "id": "TYtmslWP7AQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integrating Gemini with LangChain**"
      ],
      "metadata": {
        "id": "FlL4IweC8bsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model='gemeni-pro', temperature= 0.8)\n",
        "response = llm.invoke('what are the names of the planets in our solar system?')\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "BTmvo8lO8bAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-pro')\n",
        "\n",
        "prompt = PromptTemplate.from_template('You are a content creator. Write me a tweet about {topic}')\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topic = 'Why will AI change the world'\n",
        "response = chain.invoke(input=topic)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "H468o78VyBdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['topic'])\n",
        "print()\n",
        "print(response['text'])\n"
      ],
      "metadata": {
        "id": "fgZMn-XC27To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**System Prompt and Streaming**\n",
        "\n",
        "System Message: Unlike OpenAI’s chat completions API, Gemini doesn’t directly support a system message but can incorporate it into the first human message.\n",
        "\n",
        "\n",
        "System Prompt: Integrates the system message into the first human message to guide the AI’s behavior.\n",
        "Streaming: Allows receiving responses piece by piece, improving experience and reducing latency."
      ],
      "metadata": {
        "id": "iARbF5vo3SQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##System Prompt\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-pro', convert_system_message_to_human=True)\n",
        "\n",
        "output = llm.invoke(\n",
        "    [\n",
        "        SystemMessage(content='Answer only YES or NO in French.'),\n",
        "        HumanMessage(content='Is fish a mammal?')\n",
        "    ]\n",
        ")\n",
        "output.content\n"
      ],
      "metadata": {
        "id": "4bfFfOUL3WzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Streaming\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0)\n",
        "prompt = 'Write a scientific paper outlining the mathematical foundation of our universe.'\n",
        "response = llm.invoke(prompt)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "8ZtvtkZt4FVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream(prompt):\n",
        "    print(chunk.content, end='')\n",
        "    print('_'*100)"
      ],
      "metadata": {
        "id": "qSrg3GxS5evm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multimodal AI with Gemini Pro Vision**"
      ],
      "metadata": {
        "id": "bMdxn4-h6Fvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pillow"
      ],
      "metadata": {
        "id": "P-XYv1zO6L4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "img=Image.open('name_of_image.jpg ')"
      ],
      "metadata": {
        "id": "3TQFaCfV6eef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model='gemini-pro-vision')\n",
        "prompt = 'What is in this image?'\n",
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {'type': 'text', 'text': prompt},\n",
        "        {'type': 'image_url', 'image_url': img}\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = llm.invoke([message])\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "Kz5Egiod7KeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_gemini(text, image, model='gemini-pro-vision'):\n",
        "    llm = ChatGoogleGenerativeAI(model=model)\n",
        "    message = HumanMessage(\n",
        "        content=[\n",
        "            {'type': 'text', 'text': text},\n",
        "            {'type': 'image_url', 'image_url': image}\n",
        "        ]\n",
        "    )\n",
        "    response = llm.invoke([message])\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "ozJZxmZz8OYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ask_gemini('what is this image? How can i identify the parameters which you mentioned in this image', img)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "hqyiRJg48P83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from IPython.display import Image\n",
        "\n",
        "image_url = 'https://picsum.photos/id/40/4106/2806'\n",
        "content = requests.get(image_url).content\n",
        "image_data = Image(content)\n",
        "image_data\n"
      ],
      "metadata": {
        "id": "bLFad_hw9Jnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ask_gemini('Describe this image as detailed as possible', image_url)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "hsWlikgh9RPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini Safety Settings**\n",
        "\n",
        "Content processed through the Gemini API is evaluated against safety attributes that include harmful categories and sensitive topics. The API blocks unsafe content based on configured thresholds for these attributes.\n",
        "\n",
        "The default settings block content with a medium or high probability of being unsafe across all categories. Developers can adjust these thresholds to fit different applications. For instance, more lenient settings might be suitable for a computer game compared to a medical application.\n",
        "\n",
        "The video shows how to configure safety settings by importing the `HarmCategory` and `HarmBlockThreshold` classes from the Long Chain Google library. A new language model object is created with customized safety settings, allowing all dangerous content and blocking only severe harassment.\n",
        "\n"
      ],
      "metadata": {
        "id": "K346p5eY9i3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm1 = ChatGoogleGenerativeAI(model='gemini-pro')\n",
        "prompt = 'How to shoot an animal?'\n",
        "response = llm1.invoke(prompt)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "id": "bSOePOdm-a3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import HarmCategory, HarmBlockThreshold\n",
        "llm2 = ChatGoogleGenerativeAI(\n",
        "    model='gemini-pro',\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
        "    }\n",
        ")\n",
        "\n",
        "response =llm2.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "l4LsugmsAsSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}