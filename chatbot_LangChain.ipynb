{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMECrOaI9tBebN4xyBoqrH1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R0b0t-Maker/LLM-T/blob/main/chatbot_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3IA2GjvP77S",
        "outputId": "20c6e0b2-5e3d-4ebd-ec02-fe8bb1068f05"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/drive/MyDrive/Udemy/LangChain/p-1/requirements.txt\" -q"
      ],
      "metadata": {
        "id": "29i1DIhZXXaQ",
        "outputId": "0b9ce8b7-beb5-4983-e3a0-bfac8788e6e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip show openai"
      ],
      "metadata": {
        "id": "V0l5iguyYiiC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet openai python-dotenv\n"
      ],
      "metadata": {
        "id": "O48scuZCmsqW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(\"/content/drive/MyDrive/Udemy/LangChain/p-1/.env\", override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR_9JiDskM6Q",
        "outputId": "0b51d8e0-0b16-45ec-d9d0-ab99d225614e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "VkId88TFtCYx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm= ChatOpenAI()\n",
        "llm.invoke('What is AI model?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpPkygxOtKyp",
        "outputId": "efb04fff-a9ac-4086-c2fd-4e1d2238fba7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='An AI model is a mathematical representation or framework that identifies patterns and relationships within data, allowing it to make predictions or decisions without being explicitly programmed. AI models are trained using algorithms and data, and they can be used in various applications such as image recognition, natural language processing, and predictive analytics. Some common types of AI models include neural networks, decision trees, support vector machines, and deep learning models.', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 12, 'total_tokens': 93}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3a9c3bf4-9163-4ee4-bcf1-1db2f6671655-0')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke('What is AI model?')\n",
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UXL81b4uFBx",
        "outputId": "14032379-8b9d-45af-df1f-2e0f1fd3d903"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An AI model is a computational algorithm or system that is designed to simulate human intelligence and perform tasks that typically require human intelligence, such as learning, reasoning, problem-solving, perception, and language understanding. AI models are trained on large amounts of data to make predictions, recognize patterns, and make decisions based on that data. These models are used in various applications such as image and speech recognition, natural language processing, autonomous vehicles, and many more.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(ChatOpenAI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwFvIIlVuulR",
        "outputId": "db8ee6fb-ea21-43be-9347-00dcf8f9648a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class ChatOpenAI in module langchain_openai.chat_models.base:\n",
            "\n",
            "class ChatOpenAI(BaseChatOpenAI)\n",
            " |  ChatOpenAI(*, name: Optional[str] = None, cache: ForwardRef('Union[BaseCache, bool, None]') = None, verbose: bool = None, callbacks: ForwardRef('Callbacks') = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, custom_get_token_ids: Optional[Callable[[str], List[int]]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, client: Any = None, async_client: Any = None, model: str = 'gpt-3.5-turbo', temperature: float = 0.7, model_kwargs: Dict[str, Any] = None, api_key: Optional[pydantic.v1.types.SecretStr] = None, base_url: Optional[str] = None, organization: Optional[str] = None, openai_proxy: Optional[str] = None, timeout: Union[float, Tuple[float, float], Any, NoneType] = None, max_retries: int = 2, streaming: bool = False, n: int = 1, max_tokens: Optional[int] = None, tiktoken_model_name: Optional[str] = None, default_headers: Optional[Mapping[str, str]] = None, default_query: Optional[Mapping[str, object]] = None, http_client: Optional[Any] = None, http_async_client: Optional[Any] = None) -> None\n",
            " |  \n",
            " |  `OpenAI` Chat large language models API.\n",
            " |  \n",
            " |  To use, you should have the environment variable ``OPENAI_API_KEY``\n",
            " |  set with your API key, or pass it as a named parameter to the constructor.\n",
            " |  \n",
            " |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
            " |  in, even if not explicitly saved on this class.\n",
            " |  \n",
            " |  Example:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |  \n",
            " |          model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      ChatOpenAI\n",
            " |      BaseChatOpenAI\n",
            " |      langchain_core.language_models.chat_models.BaseChatModel\n",
            " |      langchain_core.language_models.base.BaseLanguageModel\n",
            " |      langchain_core.runnables.base.RunnableSerializable\n",
            " |      langchain_core.load.serializable.Serializable\n",
            " |      pydantic.v1.main.BaseModel\n",
            " |      pydantic.v1.utils.Representation\n",
            " |      langchain_core.runnables.base.Runnable\n",
            " |      typing.Generic\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  get_lc_namespace() -> 'List[str]' from pydantic.v1.main.ModelMetaclass\n",
            " |      Get the namespace of the langchain object.\n",
            " |  \n",
            " |  is_lc_serializable() -> 'bool' from pydantic.v1.main.ModelMetaclass\n",
            " |      Return whether this model can be serialized by Langchain.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  lc_attributes\n",
            " |      List of attribute names that should be included in the serialized kwargs.\n",
            " |      \n",
            " |      These attributes must be accepted by the constructor.\n",
            " |  \n",
            " |  lc_secrets\n",
            " |      A map of constructor argument names to secret ids.\n",
            " |      \n",
            " |      For example,\n",
            " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  __class_vars__ = set()\n",
            " |  \n",
            " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
            " |  \n",
            " |  __custom_root_type__ = False\n",
            " |  \n",
            " |  __exclude_fields__ = {'async_client': True, 'callback_manager': True, ...\n",
            " |  \n",
            " |  __fields__ = {'async_client': ModelField(name='async_client', type=Opt...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __include_fields__ = None\n",
            " |  \n",
            " |  __parameters__ = ()\n",
            " |  \n",
            " |  __post_root_validators__ = [(False, <function BaseChatModel.raise_depr...\n",
            " |  \n",
            " |  __pre_root_validators__ = [<function BaseChatOpenAI.build_extra>]\n",
            " |  \n",
            " |  __private_attributes__ = {}\n",
            " |  \n",
            " |  __schema_cache__ = {}\n",
            " |  \n",
            " |  __signature__ = <Signature (*, name: Optional[str] = None, cache...htt...\n",
            " |  \n",
            " |  __validators__ = {'verbose': [<pydantic.v1.class_validators.Validator ...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseChatOpenAI:\n",
            " |  \n",
            " |  bind_functions(self, functions: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', function_call: \"Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
            " |      Bind functions (and other objects) to this chat model.\n",
            " |      \n",
            " |      Assumes model is compatible with OpenAI function-calling API.\n",
            " |      \n",
            " |      NOTE: Using bind_tools is recommended instead, as the `functions` and\n",
            " |          `function_call` request parameters are officially marked as deprecated by\n",
            " |          OpenAI.\n",
            " |      \n",
            " |      Args:\n",
            " |          functions: A list of function definitions to bind to this chat model.\n",
            " |              Can be  a dictionary, pydantic model, or callable. Pydantic\n",
            " |              models and callables will be automatically converted to\n",
            " |              their schema dictionary representation.\n",
            " |          function_call: Which function to require the model to call.\n",
            " |              Must be the name of the single provided function or\n",
            " |              \"auto\" to automatically determine which function to call\n",
            " |              (if any).\n",
            " |          **kwargs: Any additional parameters to pass to the\n",
            " |              :class:`~langchain.runnable.Runnable` constructor.\n",
            " |  \n",
            " |  bind_tools(self, tools: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', *, tool_choice: \"Optional[Union[dict, str, Literal['auto', 'none', 'required', 'any'], bool]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
            " |      Bind tool-like objects to this chat model.\n",
            " |      \n",
            " |      Assumes model is compatible with OpenAI tool-calling API.\n",
            " |      \n",
            " |      Args:\n",
            " |          tools: A list of tool definitions to bind to this chat model.\n",
            " |              Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
            " |              models, callables, and BaseTools will be automatically converted to\n",
            " |              their schema dictionary representation.\n",
            " |          tool_choice: Which tool to require the model to call.\n",
            " |              Options are:\n",
            " |              name of the tool (str): calls corresponding tool;\n",
            " |              \"auto\": automatically selects a tool (including no tool);\n",
            " |              \"none\": does not call a tool;\n",
            " |              \"any\" or \"required\": force at least one tool to be called;\n",
            " |              True: forces tool call (requires `tools` be length 1);\n",
            " |              False: no effect;\n",
            " |      \n",
            " |              or a dict of the form:\n",
            " |              {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
            " |          **kwargs: Any additional parameters to pass to the\n",
            " |              :class:`~langchain.runnable.Runnable` constructor.\n",
            " |  \n",
            " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
            " |      Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n",
            " |      \n",
            " |      Official documentation: https://github.com/openai/openai-cookbook/blob/\n",
            " |      main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
            " |  \n",
            " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
            " |      Get the tokens present in the text with tiktoken package.\n",
            " |  \n",
            " |  with_structured_output(self, schema: 'Optional[_DictOrPydanticClass]' = None, *, method: \"Literal['function_calling', 'json_mode']\" = 'function_calling', include_raw: 'bool' = False, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, _DictOrPydantic]'\n",
            " |      Model wrapper that returns outputs formatted to match the given schema.\n",
            " |      \n",
            " |              Args:\n",
            " |                  schema: The output schema as a dict or a Pydantic class. If a Pydantic class\n",
            " |                      then the model output will be an object of that class. If a dict then\n",
            " |                      the model output will be a dict. With a Pydantic class the returned\n",
            " |                      attributes will be validated, whereas with a dict they will not be. If\n",
            " |                      `method` is \"function_calling\" and `schema` is a dict, then the dict\n",
            " |                      must match the OpenAI function-calling spec or be a valid JSON schema\n",
            " |                      with top level 'title' and 'description' keys specified.\n",
            " |                  method: The method for steering model generation, either \"function_calling\"\n",
            " |                      or \"json_mode\". If \"function_calling\" then the schema will be converted\n",
            " |                      to an OpenAI function and the returned model will make use of the\n",
            " |                      function-calling API. If \"json_mode\" then OpenAI's JSON mode will be\n",
            " |                      used. Note that if using \"json_mode\" then you must include instructions\n",
            " |                      for formatting the output into the desired schema into the model call.\n",
            " |                  include_raw: If False then only the parsed structured output is returned. If\n",
            " |                      an error occurs during model output parsing it will be raised. If True\n",
            " |                      then both the raw model response (a BaseMessage) and the parsed model\n",
            " |                      response will be returned. If an error occurs during output parsing it\n",
            " |                      will be caught and returned as well. The final output is always a dict\n",
            " |                      with keys \"raw\", \"parsed\", and \"parsing_error\".\n",
            " |      \n",
            " |              Returns:\n",
            " |                  A Runnable that takes any ChatModel input and returns as output:\n",
            " |      \n",
            " |                      If include_raw is True then a dict with keys:\n",
            " |                          raw: BaseMessage\n",
            " |                          parsed: Optional[_DictOrPydantic]\n",
            " |                          parsing_error: Optional[BaseException]\n",
            " |      \n",
            " |                      If include_raw is False then just _DictOrPydantic is returned,\n",
            " |                      where _DictOrPydantic depends on the schema:\n",
            " |      \n",
            " |                      If schema is a Pydantic class then _DictOrPydantic is the Pydantic\n",
            " |                          class.\n",
            " |      \n",
            " |                      If schema is a dict then _DictOrPydantic is a dict.\n",
            " |      \n",
            " |              Example: Function-calling, Pydantic schema (method=\"function_calling\", include_raw=False):\n",
            " |                  .. code-block:: python\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |                      from langchain_core.pydantic_v1 import BaseModel\n",
            " |      \n",
            " |                      class AnswerWithJustification(BaseModel):\n",
            " |                          '''An answer to the user question along with justification for the answer.'''\n",
            " |                          answer: str\n",
            " |                          justification: str\n",
            " |      \n",
            " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
            " |                      structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
            " |      \n",
            " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
            " |      \n",
            " |                      # -> AnswerWithJustification(\n",
            " |                      #     answer='They weigh the same',\n",
            " |                      #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
            " |                      # )\n",
            " |      \n",
            " |              Example: Function-calling, Pydantic schema (method=\"function_calling\", include_raw=True):\n",
            " |                  .. code-block:: python\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |                      from langchain_core.pydantic_v1 import BaseModel\n",
            " |      \n",
            " |                      class AnswerWithJustification(BaseModel):\n",
            " |                          '''An answer to the user question along with justification for the answer.'''\n",
            " |                          answer: str\n",
            " |                          justification: str\n",
            " |      \n",
            " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
            " |                      structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n",
            " |      \n",
            " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
            " |                      # -> {\n",
            " |                      #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n",
            " |                      #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n",
            " |                      #     'parsing_error': None\n",
            " |                      # }\n",
            " |      \n",
            " |              Example: Function-calling, dict schema (method=\"function_calling\", include_raw=False):\n",
            " |                  .. code-block:: python\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |                      from langchain_core.pydantic_v1 import BaseModel\n",
            " |                      from langchain_core.utils.function_calling import convert_to_openai_tool\n",
            " |      \n",
            " |                      class AnswerWithJustification(BaseModel):\n",
            " |                          '''An answer to the user question along with justification for the answer.'''\n",
            " |                          answer: str\n",
            " |                          justification: str\n",
            " |      \n",
            " |                      dict_schema = convert_to_openai_tool(AnswerWithJustification)\n",
            " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
            " |                      structured_llm = llm.with_structured_output(dict_schema)\n",
            " |      \n",
            " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
            " |                      # -> {\n",
            " |                      #     'answer': 'They weigh the same',\n",
            " |                      #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
            " |                      # }\n",
            " |      \n",
            " |              Example: JSON mode, Pydantic schema (method=\"json_mode\", include_raw=True):\n",
            " |                  .. code-block::\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |                      from langchain_core.pydantic_v1 import BaseModel\n",
            " |      \n",
            " |                      class AnswerWithJustification(BaseModel):\n",
            " |                          answer: str\n",
            " |                          justification: str\n",
            " |      \n",
            " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
            " |                      structured_llm = llm.with_structured_output(\n",
            " |                          AnswerWithJustification,\n",
            " |                          method=\"json_mode\",\n",
            " |                          include_raw=True\n",
            " |                      )\n",
            " |      \n",
            " |                      structured_llm.invoke(\n",
            " |                          \"Answer the following question. \"\n",
            " |                          \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\n",
            " |      \n",
            " |      \"\n",
            " |                          \"What's heavier a pound of bricks or a pound of feathers?\"\n",
            " |                      )\n",
            " |                      # -> {\n",
            " |                      #     'raw': AIMessage(content='{\n",
            " |          \"answer\": \"They are both the same weight.\",\n",
            " |          \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \n",
            " |      }'),\n",
            " |                      #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n",
            " |                      #     'parsing_error': None\n",
            " |                      # }\n",
            " |      \n",
            " |              Example: JSON mode, no schema (schema=None, method=\"json_mode\", include_raw=True):\n",
            " |                  .. code-block::\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |      \n",
            " |                      structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n",
            " |      \n",
            " |                      structured_llm.invoke(\n",
            " |                          \"Answer the following question. \"\n",
            " |                          \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\n",
            " |      \n",
            " |      \"\n",
            " |                          \"What's heavier a pound of bricks or a pound of feathers?\"\n",
            " |                      )\n",
            " |                      # -> {\n",
            " |                      #     'raw': AIMessage(content='{\n",
            " |          \"answer\": \"They are both the same weight.\",\n",
            " |          \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \n",
            " |      }'),\n",
            " |                      #     'parsed': {\n",
            " |                      #         'answer': 'They are both the same weight.',\n",
            " |                      #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n",
            " |                      #     },\n",
            " |                      #     'parsing_error': None\n",
            " |                      # }\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from BaseChatOpenAI:\n",
            " |  \n",
            " |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.v1.main.ModelMetaclass\n",
            " |      Build extra kwargs from additional params that were passed in.\n",
            " |  \n",
            " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
            " |      Validate that api key and python package exists in environment.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from BaseChatOpenAI:\n",
            " |  \n",
            " |  Config = <class 'langchain_openai.chat_models.base.BaseChatOpenAI.Conf...\n",
            " |      Configuration for this pydantic object.\n",
            " |  \n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |  \n",
            " |  __call__(self, messages: 'List[BaseMessage]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use invoke instead.\n",
            " |  \n",
            " |  async agenerate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          messages: List of list of messages.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts and return model generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
            " |              converted to match the format of any language model (string for pure\n",
            " |              text generation models and BaseMessages for chat models).\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      Default implementation of ainvoke, calls invoke from a thread.\n",
            " |      \n",
            " |      The default implementation allows usage of async code even if\n",
            " |      the runnable did not implement a native async version of invoke.\n",
            " |      \n",
            " |      Subclasses should override this method if they can run asynchronously.\n",
            " |  \n",
            " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use ainvoke instead.\n",
            " |  \n",
            " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use ainvoke instead.\n",
            " |  \n",
            " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
            " |      Default implementation of astream, which calls ainvoke.\n",
            " |      Subclasses should override this method if they support streaming output.\n",
            " |  \n",
            " |  call_as_llm(self, message: 'str', stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use invoke instead.\n",
            " |  \n",
            " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
            " |      Return a dictionary of the LLM.\n",
            " |  \n",
            " |  generate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          messages: List of list of messages.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
            " |              converted to match the format of any language model (string for pure\n",
            " |              text generation models and BaseMessages for chat models).\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      Transform a single input into an output. Override to implement.\n",
            " |      \n",
            " |      Args:\n",
            " |          input: The input to the runnable.\n",
            " |          config: A config to use when invoking the runnable.\n",
            " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
            " |             purposes, 'max_concurrency' for controlling how much work to do\n",
            " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
            " |             for more details.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The output of the runnable.\n",
            " |  \n",
            " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use invoke instead.\n",
            " |  \n",
            " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use invoke instead.\n",
            " |  \n",
            " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
            " |      Default implementation of stream, which calls invoke.\n",
            " |      Subclasses should override this method if they support streaming output.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |  \n",
            " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
            " |      Raise deprecation warning if callback_manager is used.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |  \n",
            " |  OutputType\n",
            " |      Get the output type for this runnable.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |  \n",
            " |  __orig_bases__ = (langchain_core.language_models.base.BaseLanguageMode...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |  \n",
            " |  get_num_tokens(self, text: 'str') -> 'int'\n",
            " |      Get the number of tokens present in the text.\n",
            " |      \n",
            " |      Useful for checking if an input will fit in a model's context window.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: The string input to tokenize.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The integer number of tokens in the text.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |  \n",
            " |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.v1.main.ModelMetaclass\n",
            " |      If verbose is None, set it.\n",
            " |      \n",
            " |      This allows users to pass in None as verbose to access the global setting.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |  \n",
            " |  InputType\n",
            " |      Get the input type for this runnable.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
            " |  \n",
            " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
            " |      Configure alternatives for runnables that can be set at runtime.\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_anthropic import ChatAnthropic\n",
            " |          from langchain_core.runnables.utils import ConfigurableField\n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |      \n",
            " |          model = ChatAnthropic(\n",
            " |              model_name=\"claude-3-sonnet-20240229\"\n",
            " |          ).configurable_alternatives(\n",
            " |              ConfigurableField(id=\"llm\"),\n",
            " |              default_key=\"anthropic\",\n",
            " |              openai=ChatOpenAI()\n",
            " |          )\n",
            " |      \n",
            " |          # uses the default model ChatAnthropic\n",
            " |          print(model.invoke(\"which organization created you?\").content)\n",
            " |      \n",
            " |          # uses ChatOpenaAI\n",
            " |          print(\n",
            " |              model.with_config(\n",
            " |                  configurable={\"llm\": \"openai\"}\n",
            " |              ).invoke(\"which organization created you?\").content\n",
            " |          )\n",
            " |  \n",
            " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
            " |      Configure particular runnable fields at runtime.\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_core.runnables import ConfigurableField\n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |      \n",
            " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
            " |              max_tokens=ConfigurableField(\n",
            " |                  id=\"output_token_number\",\n",
            " |                  name=\"Max tokens in the output\",\n",
            " |                  description=\"The maximum number of tokens in the output\",\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |          # max_tokens = 20\n",
            " |          print(\n",
            " |              \"max_tokens_20: \",\n",
            " |              model.invoke(\"tell me something about chess\").content\n",
            " |          )\n",
            " |      \n",
            " |          # max_tokens = 200\n",
            " |          print(\"max_tokens_200: \", model.with_config(\n",
            " |              configurable={\"output_token_number\": 200}\n",
            " |              ).invoke(\"tell me something about chess\").content\n",
            " |          )\n",
            " |  \n",
            " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
            " |      Serialize the runnable to JSON.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
            " |  \n",
            " |  __repr_args__(self) -> Any\n",
            " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
            " |      \n",
            " |      Can either return:\n",
            " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
            " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
            " |  \n",
            " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
            " |  \n",
            " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
            " |      A unique identifier for this class for serialization purposes.\n",
            " |      \n",
            " |      The unique identifier is a list of strings that describes the path\n",
            " |      to the object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __getstate__(self) -> 'DictAny'\n",
            " |  \n",
            " |  __init__(__pydantic_self__, **data: Any) -> None\n",
            " |      Create a new model by parsing and validating input data from keyword arguments.\n",
            " |      \n",
            " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
            " |  \n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      so `dict(model)` works\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state: 'DictAny') -> None\n",
            " |  \n",
            " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
            " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
            " |      \n",
            " |      :param include: fields to include in new model\n",
            " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
            " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
            " |          the new model: you should trust this data\n",
            " |      :param deep: set to `True` to make a deep copy of the model\n",
            " |      :return: new model instance\n",
            " |  \n",
            " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
            " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
            " |      \n",
            " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
            " |      Same as update_forward_refs but will not raise exception\n",
            " |      when forward references are not defined.\n",
            " |  \n",
            " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
            " |  \n",
            " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
            " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
            " |  \n",
            " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __fields_set__\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.v1.utils.Representation:\n",
            " |  \n",
            " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __repr_name__(self) -> str\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |  \n",
            " |  __repr_str__(self, join_str: str) -> str\n",
            " |  \n",
            " |  __rich_repr__(self) -> 'RichReprResult'\n",
            " |      Get fields for Rich library\n",
            " |  \n",
            " |  __str__(self) -> str\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
            " |  \n",
            " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
            " |      Compose this runnable with another object to create a RunnableSequence.\n",
            " |  \n",
            " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
            " |      Compose this runnable with another object to create a RunnableSequence.\n",
            " |  \n",
            " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
            " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
            " |      \n",
            " |      The default implementation of batch works well for IO bound runnables.\n",
            " |      \n",
            " |      Subclasses should override this method if they can batch more efficiently;\n",
            " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
            " |  \n",
            " |  async abatch_as_completed(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Tuple[int, Union[Output, Exception]]]'\n",
            " |      Run ainvoke in parallel on a list of inputs,\n",
            " |      yielding results as they complete.\n",
            " |  \n",
            " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
            " |      Assigns new fields to the dict output of this runnable.\n",
            " |      Returns a new runnable.\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
            " |          from langchain_core.output_parsers import StrOutputParser\n",
            " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
            " |          from langchain_core.runnables import Runnable\n",
            " |          from operator import itemgetter\n",
            " |      \n",
            " |          prompt = (\n",
            " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
            " |              + \"{question}\"\n",
            " |          )\n",
            " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
            " |      \n",
            " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
            " |      \n",
            " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
            " |      \n",
            " |          print(chain_with_assign.input_schema.schema())\n",
            " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
            " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
            " |          print(chain_with_assign.output_schema.schema()) #\n",
            " |          {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
            " |          {'str': {'title': 'Str',\n",
            " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
            " |  \n",
            " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
            " |      [*Beta*] Generate a stream of events.\n",
            " |      \n",
            " |      Use to create an iterator over StreamEvents that provide real-time information\n",
            " |      about the progress of the runnable, including StreamEvents from intermediate\n",
            " |      results.\n",
            " |      \n",
            " |      A StreamEvent is a dictionary with the following schema:\n",
            " |      \n",
            " |      - ``event``: **str** - Event names are of the\n",
            " |          format: on_[runnable_type]_(start|stream|end).\n",
            " |      - ``name``: **str** - The name of the runnable that generated the event.\n",
            " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
            " |          the runnable that emitted the event.\n",
            " |          A child runnable that gets invoked as part of the execution of a\n",
            " |          parent runnable is assigned its own unique ID.\n",
            " |      - ``tags``: **Optional[List[str]]** - The tags of the runnable that generated\n",
            " |          the event.\n",
            " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the runnable\n",
            " |          that generated the event.\n",
            " |      - ``data``: **Dict[str, Any]**\n",
            " |      \n",
            " |      \n",
            " |      Below is a table that illustrates some evens that might be emitted by various\n",
            " |      chains. Metadata fields have been omitted from the table for brevity.\n",
            " |      Chain definitions have been included after the table.\n",
            " |      \n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
            " |      +======================+==================+=================================+===============================================+=================================================+\n",
            " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      \n",
            " |      Here are declarations associated with the events shown above:\n",
            " |      \n",
            " |      `format_docs`:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          def format_docs(docs: List[Document]) -> str:\n",
            " |              '''Format the docs.'''\n",
            " |              return \", \".join([doc.page_content for doc in docs])\n",
            " |      \n",
            " |          format_docs = RunnableLambda(format_docs)\n",
            " |      \n",
            " |      `some_tool`:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          @tool\n",
            " |          def some_tool(x: int, y: str) -> dict:\n",
            " |              '''Some_tool.'''\n",
            " |              return {\"x\": x, \"y\": y}\n",
            " |      \n",
            " |      `prompt`:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          template = ChatPromptTemplate.from_messages(\n",
            " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
            " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
            " |      \n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |      \n",
            " |          async def reverse(s: str) -> str:\n",
            " |              return s[::-1]\n",
            " |      \n",
            " |          chain = RunnableLambda(func=reverse)\n",
            " |      \n",
            " |          events = [\n",
            " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
            " |          ]\n",
            " |      \n",
            " |          # will produce the following events (run_id has been omitted for brevity):\n",
            " |          [\n",
            " |              {\n",
            " |                  \"data\": {\"input\": \"hello\"},\n",
            " |                  \"event\": \"on_chain_start\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |              {\n",
            " |                  \"data\": {\"chunk\": \"olleh\"},\n",
            " |                  \"event\": \"on_chain_stream\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |              {\n",
            " |                  \"data\": {\"output\": \"olleh\"},\n",
            " |                  \"event\": \"on_chain_end\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |          ]\n",
            " |      \n",
            " |      Args:\n",
            " |          input: The input to the runnable.\n",
            " |          config: The config to use for the runnable.\n",
            " |          version: The version of the schema to use.\n",
            " |                   Currently only version 1 is available.\n",
            " |                   No default will be assigned until the API is stabilized.\n",
            " |          include_names: Only include events from runnables with matching names.\n",
            " |          include_types: Only include events from runnables with matching types.\n",
            " |          include_tags: Only include events from runnables with matching tags.\n",
            " |          exclude_names: Exclude events from runnables with matching names.\n",
            " |          exclude_types: Exclude events from runnables with matching types.\n",
            " |          exclude_tags: Exclude events from runnables with matching tags.\n",
            " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
            " |              These will be passed to astream_log as this implementation\n",
            " |              of astream_events is built on top of astream_log.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An async stream of StreamEvents.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. beta::\n",
            " |         This API is in beta and may change in the future.\n",
            " |  \n",
            " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
            " |      Stream all output from a runnable, as reported to the callback system.\n",
            " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
            " |      \n",
            " |      Output is streamed as Log objects, which include a list of\n",
            " |      jsonpatch ops that describe how the state of the run has changed in each\n",
            " |      step, and the final state of the run.\n",
            " |      \n",
            " |      The jsonpatch ops can be applied in order to construct state.\n",
            " |      \n",
            " |      Args:\n",
            " |          input: The input to the runnable.\n",
            " |          config: The config to use for the runnable.\n",
            " |          diff: Whether to yield diffs between each step, or the current state.\n",
            " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
            " |          include_names: Only include logs with these names.\n",
            " |          include_types: Only include logs with these types.\n",
            " |          include_tags: Only include logs with these tags.\n",
            " |          exclude_names: Exclude logs with these names.\n",
            " |          exclude_types: Exclude logs with these types.\n",
            " |          exclude_tags: Exclude logs with these tags.\n",
            " |  \n",
            " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
            " |      Default implementation of atransform, which buffers input and calls astream.\n",
            " |      Subclasses should override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |  \n",
            " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
            " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
            " |      \n",
            " |      The default implementation of batch works well for IO bound runnables.\n",
            " |      \n",
            " |      Subclasses should override this method if they can batch more efficiently;\n",
            " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
            " |  \n",
            " |  batch_as_completed(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[Tuple[int, Union[Output, Exception]]]'\n",
            " |      Run invoke in parallel on a list of inputs,\n",
            " |      yielding results as they complete.\n",
            " |  \n",
            " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind arguments to a Runnable, returning a new Runnable.\n",
            " |      \n",
            " |      Useful when a runnable in a chain requires an argument that is not\n",
            " |      in the output of the previous runnable or included in the user input.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_community.chat_models import ChatOllama\n",
            " |          from langchain_core.output_parsers import StrOutputParser\n",
            " |      \n",
            " |          llm = ChatOllama(model='llama2')\n",
            " |      \n",
            " |          # Without bind.\n",
            " |          chain = (\n",
            " |              llm\n",
            " |              | StrOutputParser()\n",
            " |          )\n",
            " |      \n",
            " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
            " |          # Output is 'One two three four five.'\n",
            " |      \n",
            " |          # With bind.\n",
            " |          chain = (\n",
            " |              llm.bind(stop=[\"three\"])\n",
            " |              | StrOutputParser()\n",
            " |          )\n",
            " |      \n",
            " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
            " |          # Output is 'One two'\n",
            " |  \n",
            " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
            " |      The type of config this runnable accepts specified as a pydantic model.\n",
            " |      \n",
            " |      To mark a field as configurable, see the `configurable_fields`\n",
            " |      and `configurable_alternatives` methods.\n",
            " |      \n",
            " |      Args:\n",
            " |          include: A list of fields to include in the config schema.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate config.\n",
            " |  \n",
            " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
            " |      Return a graph representation of this runnable.\n",
            " |  \n",
            " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
            " |      Get a pydantic model that can be used to validate input to the runnable.\n",
            " |      \n",
            " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
            " |      methods will have a dynamic input schema that depends on which\n",
            " |      configuration the runnable is invoked with.\n",
            " |      \n",
            " |      This method allows to get an input schema for a specific configuration.\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate input.\n",
            " |  \n",
            " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
            " |      Get the name of the runnable.\n",
            " |  \n",
            " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
            " |      Get a pydantic model that can be used to validate output to the runnable.\n",
            " |      \n",
            " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
            " |      methods will have a dynamic output schema that depends on which\n",
            " |      configuration the runnable is invoked with.\n",
            " |      \n",
            " |      This method allows to get an output schema for a specific configuration.\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate output.\n",
            " |  \n",
            " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
            " |  \n",
            " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
            " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
            " |      by calling invoke() with each input.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |                  from langchain_core.runnables import RunnableLambda\n",
            " |      \n",
            " |                  def _lambda(x: int) -> int:\n",
            " |                      return x + 1\n",
            " |      \n",
            " |                  runnable = RunnableLambda(_lambda)\n",
            " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
            " |  \n",
            " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
            " |      Pick keys from the dict output of this runnable.\n",
            " |      \n",
            " |      Pick single key:\n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |              import json\n",
            " |      \n",
            " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
            " |      \n",
            " |              as_str = RunnableLambda(str)\n",
            " |              as_json = RunnableLambda(json.loads)\n",
            " |              chain = RunnableMap(str=as_str, json=as_json)\n",
            " |      \n",
            " |              chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
            " |      \n",
            " |              json_only_chain = chain.pick(\"json\")\n",
            " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> [1, 2, 3]\n",
            " |      \n",
            " |      Pick list of keys:\n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |              from typing import Any\n",
            " |      \n",
            " |              import json\n",
            " |      \n",
            " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
            " |      \n",
            " |              as_str = RunnableLambda(str)\n",
            " |              as_json = RunnableLambda(json.loads)\n",
            " |              def as_bytes(x: Any) -> bytes:\n",
            " |                  return bytes(x, \"utf-8\")\n",
            " |      \n",
            " |              chain = RunnableMap(\n",
            " |                  str=as_str,\n",
            " |                  json=as_json,\n",
            " |                  bytes=RunnableLambda(as_bytes)\n",
            " |              )\n",
            " |      \n",
            " |              chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
            " |      \n",
            " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
            " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
            " |  \n",
            " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
            " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
            " |      \n",
            " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
            " |      \n",
            " |      Example:\n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |              from langchain_core.runnables import RunnableLambda\n",
            " |      \n",
            " |              def add_one(x: int) -> int:\n",
            " |                  return x + 1\n",
            " |      \n",
            " |              def mul_two(x: int) -> int:\n",
            " |                  return x * 2\n",
            " |      \n",
            " |              runnable_1 = RunnableLambda(add_one)\n",
            " |              runnable_2 = RunnableLambda(mul_two)\n",
            " |              sequence = runnable_1.pipe(runnable_2)\n",
            " |              # Or equivalently:\n",
            " |              # sequence = runnable_1 | runnable_2\n",
            " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
            " |              sequence.invoke(1)\n",
            " |              await sequence.ainvoke(1)\n",
            " |              # -> 4\n",
            " |      \n",
            " |              sequence.batch([1, 2, 3])\n",
            " |              await sequence.abatch([1, 2, 3])\n",
            " |              # -> [4, 6, 8]\n",
            " |  \n",
            " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
            " |      Default implementation of transform, which buffers input and then calls stream.\n",
            " |      Subclasses should override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |  \n",
            " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind config to a Runnable, returning a new Runnable.\n",
            " |  \n",
            " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
            " |      Add fallbacks to a runnable, returning a new Runnable.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |              from typing import Iterator\n",
            " |      \n",
            " |              from langchain_core.runnables import RunnableGenerator\n",
            " |      \n",
            " |      \n",
            " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
            " |                  raise ValueError()\n",
            " |                  yield \"\"\n",
            " |      \n",
            " |      \n",
            " |              def _generate(input: Iterator) -> Iterator[str]:\n",
            " |                  yield from \"foo bar\"\n",
            " |      \n",
            " |      \n",
            " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
            " |                  [RunnableGenerator(_generate)]\n",
            " |                  )\n",
            " |              print(''.join(runnable.stream({}))) #foo bar\n",
            " |      \n",
            " |      Args:\n",
            " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
            " |          exceptions_to_handle: A tuple of exception types to handle.\n",
            " |          exception_key: If string is specified then handled exceptions will be passed\n",
            " |              to fallbacks as part of the input under the specified key. If None,\n",
            " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
            " |              and its fallbacks must accept a dictionary as input.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A new Runnable that will try the original runnable, and then each\n",
            " |          fallback in order, upon failures.\n",
            " |  \n",
            " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
            " |      \n",
            " |      on_start: Called before the runnable starts running, with the Run object.\n",
            " |      on_end: Called after the runnable finishes running, with the Run object.\n",
            " |      on_error: Called if the runnable throws an error, with the Run object.\n",
            " |      \n",
            " |      The Run object contains information about the run, including its id,\n",
            " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
            " |      added to the run.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |          import time\n",
            " |      \n",
            " |          def test_runnable(time_to_sleep : int):\n",
            " |              time.sleep(time_to_sleep)\n",
            " |      \n",
            " |          def fn_start(run_obj : Runnable):\n",
            " |              print(\"start_time:\", run_obj.start_time)\n",
            " |      \n",
            " |          def fn_end(run_obj : Runnable):\n",
            " |              print(\"end_time:\", run_obj.end_time)\n",
            " |      \n",
            " |          RunnableLambda(test_runnable).with_listeners(\n",
            " |              on_start=fn_start,\n",
            " |              on_end=fn_end\n",
            " |              ).invoke(2)\n",
            " |  \n",
            " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
            " |      Create a new Runnable that retries the original runnable on exceptions.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |      \n",
            " |          count = 0\n",
            " |      \n",
            " |      \n",
            " |          def _lambda(x: int) -> None:\n",
            " |              global count\n",
            " |              count = count + 1\n",
            " |              if x == 1:\n",
            " |                  raise ValueError(\"x is 1\")\n",
            " |              else:\n",
            " |                   pass\n",
            " |      \n",
            " |      \n",
            " |          runnable = RunnableLambda(_lambda)\n",
            " |          try:\n",
            " |              runnable.with_retry(\n",
            " |                  stop_after_attempt=2,\n",
            " |                  retry_if_exception_type=(ValueError,),\n",
            " |              ).invoke(1)\n",
            " |          except ValueError:\n",
            " |              pass\n",
            " |      \n",
            " |          assert (count == 2)\n",
            " |      \n",
            " |      \n",
            " |      Args:\n",
            " |          retry_if_exception_type: A tuple of exception types to retry on\n",
            " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
            " |                                   between retries\n",
            " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
            " |      \n",
            " |      Returns:\n",
            " |          A new Runnable that retries the original runnable on exceptions.\n",
            " |  \n",
            " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
            " |  \n",
            " |  config_specs\n",
            " |      List configurable fields for this runnable.\n",
            " |  \n",
            " |  input_schema\n",
            " |      The type of input this runnable accepts specified as a pydantic model.\n",
            " |  \n",
            " |  output_schema\n",
            " |      The type of output this runnable produces specified as a pydantic model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
            " |  \n",
            " |  name = None\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_2=llm.invoke('What is AI model?', model = 'gpt-4-turbo-preview')\n",
        "print(output_2.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nij74sPWvasQ",
        "outputId": "0425bbe1-a533-4ccc-ff96-749ffb1066a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An AI model refers to a mathematical framework or computational algorithm that is designed to perform tasks that would require human intelligence. These tasks can include decision-making, pattern recognition, language understanding, and visual perception, among others. AI models are built using various techniques from artificial intelligence, machine learning, and deep learning.\n",
            "\n",
            "The process of creating an AI model typically involves collecting and preparing data, selecting an appropriate algorithm or set of algorithms, training the model on the data, and then evaluating its performance. During training, the model learns to make predictions or decisions based on the input data it receives, effectively \"learning\" from the data.\n",
            "\n",
            "There are many types of AI models, each suited for different tasks. Some of the most common include:\n",
            "\n",
            "1. **Neural Networks and Deep Learning Models:** These are inspired by the structure and function of the human brain and are particularly powerful for tasks involving image and speech recognition, natural language processing, and complex pattern recognition.\n",
            "\n",
            "2. **Decision Trees and Random Forests:** Useful for classification and regression tasks. They model decisions and their possible consequences as a tree structure.\n",
            "\n",
            "3. **Support Vector Machines (SVM):** A supervised learning model that is used for classification and regression tasks. It works by finding the hyperplane that best separates different classes in the input feature space.\n",
            "\n",
            "4. **Reinforcement Learning Models:** These models learn to make decisions by taking actions in an environment to achieve some goals. They are particularly useful for robotics, gaming, and navigation tasks.\n",
            "\n",
            "5. **Generative Adversarial Networks (GANs):** Used for generating new data instances that are similar to but distinct from the training data. They are particularly known for their application in generating realistic images, art, and even music.\n",
            "\n",
            "AI models are increasingly becoming integral parts of various technologies and applications, including autonomous vehicles, recommendation systems, healthcare diagnostics, financial trading algorithms, and more, due to their ability to process and analyze vast amounts of data far more quickly and accurately than human beings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage (content = 'You are an expert crypto trader and you answer the question percisely and meticolously.'),\n",
        "    HumanMessage (content = 'What is the future of the cypto market?')\n",
        "    ]\n",
        "\n",
        "output_3 = llm.invoke (messages)\n",
        "print(output_3.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75BfUkCXw0XA",
        "outputId": "0fb68fbe-d803-4f8a-f613-2c63d85eed92"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting the future of the crypto market is inherently uncertain due to its volatile nature and various factors that can influence its direction. However, there are several trends and developments that could shape the future of the crypto market:\n",
            "\n",
            "1. **Increased Institutional Adoption:** More institutional investors are entering the crypto space, which could lead to greater stability and mainstream acceptance of cryptocurrencies.\n",
            "\n",
            "2. **Regulatory Environment:** Regulatory clarity and frameworks are still evolving globally, and regulatory developments could significantly impact the crypto market in the future.\n",
            "\n",
            "3. **Technological Advancements:** Continued innovation in blockchain technology could lead to the development of new use cases for cryptocurrencies and drive further adoption.\n",
            "\n",
            "4. **Market Maturation:** As the market matures, we may see a shift towards more established cryptocurrencies and projects with real-world utility, potentially leading to increased stability.\n",
            "\n",
            "5. **Market Cycles:** The crypto market is known for its boom-and-bust cycles, so it's important to consider that future price movements could be influenced by market cycles.\n",
            "\n",
            "6. **Macro-economic Factors:** External economic factors, such as inflation, monetary policy changes, and geopolitical events, could also impact the crypto market.\n",
            "\n",
            "In conclusion, while it's challenging to definitively predict the future of the crypto market, it's essential to stay informed about market trends, technological advancements, regulatory developments, and macro-economic factors to make informed decisions as a crypto trader. Diversification, risk management, and staying updated on market developments are crucial strategies for navigating the dynamic crypto market landscape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Memory Cache"
      ],
      "metadata": {
        "id": "Dn2OODLW4GbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model= 'gpt-3.5-turbo-instruct')"
      ],
      "metadata": {
        "id": "4JqxyEM84OGO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# measure the execution time\n",
        "%%time\n",
        "from langchain.cache import InMemoryCache\n",
        "set_llm_cache(InMemoryCache())\n",
        "prompt = 'what is a robot?'\n",
        "llm.invoke (prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "l-flMBYW4-0t",
        "outputId": "f98872a5-ba65-4677-f014-272e5466684b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 422 ms, sys: 30.2 ms, total: 452 ms\n",
            "Wall time: 2 s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nA robot is a machine that is programmed to perform a variety of tasks automatically. It is typically controlled by a computer or electronic circuit and can be designed to move, interact with its environment, and perform a specific set of tasks. Robots are used in various industries, such as manufacturing, healthcare, and transportation, to increase efficiency and productivity. They can also be used for tasks that are dangerous or difficult for humans to perform.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "llm.invoke (prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "NEp5ImIx6o7F",
        "outputId": "0e590aba-1f84-4459-fb1b-f4d557417038"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 750 µs, sys: 0 ns, total: 750 µs\n",
            "Wall time: 759 µs\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nA robot is a machine that is programmed to perform a variety of tasks automatically. It is typically controlled by a computer or electronic circuit and can be designed to move, interact with its environment, and perform a specific set of tasks. Robots are used in various industries, such as manufacturing, healthcare, and transportation, to increase efficiency and productivity. They can also be used for tasks that are dangerous or difficult for humans to perform.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQLite chching"
      ],
      "metadata": {
        "id": "3um07Zpt66Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "set_llm_cache (SQLiteCache(database_path= '.langchain.db' ))\n",
        "\n",
        "# longer time response\n",
        "llm.invoke (prompt)\n",
        "\n",
        "# shorter time response\n",
        "\n",
        "llm.invoke (prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9sRWLmvn640k",
        "outputId": "2fcb5bdf-0dca-4b34-fcf7-c412d4b863e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nA robot is a machine that is designed to carry out specific tasks, either autonomously or under the control of a human operator. It is typically equipped with sensors, actuators, and a processing unit to perceive its environment, make decisions, and perform physical actions. Robots are used in a variety of industries and applications, from manufacturing and healthcare to space exploration and entertainment. They can range from simple machines that perform repetitive tasks to advanced systems with artificial intelligence and human-like capabilities.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Streaming"
      ],
      "metadata": {
        "id": "ddmyxkgEQqnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "prompt = 'Write a lovely song'\n",
        "print(llm.invoke(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtYaM6tXQs2t",
        "outputId": "e74dac75-c35e-42f4-c256-656a79acd0b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Verse 1:\n",
            "In this world of chaos and pain\n",
            "You came into my life like a ray of light\n",
            "With your smile, you ease my strain\n",
            "And make everything feel just right\n",
            "\n",
            "Pre-Chorus:\n",
            "Your love is like a warm embrace\n",
            "That I never want to let go\n",
            "You fill my heart with so much grace\n",
            "And I just wanted you to know\n",
            "\n",
            "Chorus:\n",
            "You're my sunshine on a rainy day\n",
            "My shelter when the storms roll in\n",
            "With you, I never have to stray\n",
            "Cause your love is my safe haven\n",
            "\n",
            "Verse 2:\n",
            "Every moment spent with you\n",
            "Feels like a dream, so surreal\n",
            "You make all my dreams come true\n",
            "And with you, I know it's real\n",
            "\n",
            "Pre-Chorus:\n",
            "Your love is like a symphony\n",
            "That plays a beautiful tune\n",
            "With you, I feel so free\n",
            "And I never want to lose\n",
            "\n",
            "Chorus:\n",
            "You're my sunshine on a rainy day\n",
            "My shelter when the storms roll in\n",
            "With you, I never have to stray\n",
            "Cause your love is my safe haven\n",
            "\n",
            "Bridge:\n",
            "I never knew love until you\n",
            "You showed me what it truly means\n",
            "With you, I feel like I can do\n",
            "Anything and everything\n",
            "\n",
            "Chorus:\n",
            "You're\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream(prompt):\n",
        "  print(chunk, end = '', flush = True)"
      ],
      "metadata": {
        "id": "8GA5FIMZTX5V",
        "outputId": "6e3e8d71-2c15-4273-f1d8-f811cf139a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Verse 1:\n",
            "Sitting here under the stars\n",
            "I can feel your love from afar\n",
            "The way you look into my eyes\n",
            "It's like the world just passes by\n",
            "\n",
            "Pre-Chorus:\n",
            "You make me feel alive\n",
            "With you, I can always thrive\n",
            "No matter what the world may say\n",
            "I'll love you every single day\n",
            "\n",
            "Chorus:\n",
            "You are my everything\n",
            "My heart, my soul, my everything\n",
            "With you, I feel like I can fly\n",
            "You are the reason for my smile\n",
            "\n",
            "Verse 2:\n",
            "Every moment spent with you\n",
            "Feels like a dream come true\n",
            "Your touch, your kiss, your embrace\n",
            "It's like I'm in a magical place\n",
            "\n",
            "Pre-Chorus:\n",
            "You make me feel complete\n",
            "With you, I can conquer defeat\n",
            "No matter what life may bring\n",
            "With you, I can do anything\n",
            "\n",
            "Chorus:\n",
            "You are my everything\n",
            "My heart, my soul, my everything\n",
            "With you, I feel like I can fly\n",
            "You are the reason for my smile\n",
            "\n",
            "Bridge:\n",
            "Through the ups and downs we'll face\n",
            "Together, we'll find our way\n",
            "In your arms, I feel so strong\n",
            "With you, I know I belong\n",
            "\n",
            "Chorus:\n",
            "You are my everything\n",
            "My"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PromptTemplate"
      ],
      "metadata": {
        "id": "5FaTJjBBVyXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "template = ''' You are an expert in Robotics software developing. Explain the {Robot} application in {industry} industry.'''\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "prompt = prompt_template.format(Robot = 'delta', industry = 'healthcare')\n",
        "prompt"
      ],
      "metadata": {
        "id": "5-Zx3ouvV3oI",
        "outputId": "3d760276-2619-4721-915d-47ae65eefcf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' You are an expert in Robotics software developing. Explain the delta application in healthcare industry.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature = 0.4)\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "_RnBsNp3YFl8",
        "outputId": "0fd0a362-f97b-4e56-b70b-c0cebd9cb757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the healthcare industry, delta robots are often used in various applications to improve efficiency, accuracy, and precision. Delta robots are known for their high-speed capabilities and are often used in tasks that require quick and precise movements, such as in surgical procedures, drug dispensing, and laboratory automation.\n",
            "\n",
            "One common application of delta robots in healthcare is in surgical robotics. Delta robots can be used to assist surgeons in performing minimally invasive surgeries with high precision and accuracy. These robots can be programmed to perform delicate tasks such as suturing, tissue manipulation, and cutting with minimal invasiveness, reducing the risk of complications and improving patient outcomes.\n",
            "\n",
            "Delta robots are also used in pharmaceutical manufacturing and drug dispensing. These robots can be programmed to handle and dispense medications with high accuracy and speed, reducing the risk of errors and ensuring patients receive the correct dosage of medication. Additionally, delta robots can be used in laboratory automation to handle and process samples quickly and efficiently, improving the speed and accuracy of diagnostic testing.\n",
            "\n",
            "Overall, delta robots play a crucial role in the healthcare industry by improving efficiency, accuracy, and precision in various applications, ultimately leading to better patient outcomes and quality of care.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatPromptTemplate"
      ],
      "metadata": {
        "id": "NL90TG7QZkbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content = 'You respond only in JSON format'),\n",
        "    HumanMessagePromptTemplate.from_template ('The most famous four {robot} in {field} field.')\n",
        "])\n",
        "\n",
        "message = chat_template.format_messages(robot = 'drone', field ='healthcare')\n",
        "print (message)"
      ],
      "metadata": {
        "id": "WHkraWfjZnRr",
        "outputId": "3583fe27-f6eb-4bad-e16f-b7fb0f339e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SystemMessage(content='You respond only in JSON format'), HumanMessage(content='The most famous four drone in healthcare field.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output= llm.invoke(message)\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "_0dRo7iCdA3K",
        "outputId": "ab8e5f99-79dd-4ffb-c0df-c2e668b066e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"drones\": [\n",
            "        {\n",
            "            \"name\": \"Zipline\",\n",
            "            \"application\": \"Medical supply delivery\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"DJI Agras MG-1\",\n",
            "            \"application\": \"Crop spraying for agriculture, including medical plant cultivation\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Vayu\",\n",
            "            \"application\": \"Blood sample transportation\"\n",
            "        },\n",
            "        {\n",
            "            \"name\": \"Matternet M2\",\n",
            "            \"application\": \"Medical sample and specimen transportation\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Chain"
      ],
      "metadata": {
        "id": "3eu79-FeiesB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import LLMChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "template = ''' You are an expert in Robotics software developing. Explain the {Robot} application in {industry} industry.'''\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = LLMChain(\n",
        "\n",
        "                 llm = llm,\n",
        "                 prompt = prompt_template,\n",
        "                 verbose = True\n",
        "\n",
        ")\n",
        "\n",
        "output = chain.invoke ({'Robot':'drone', 'industry':'healthcare'})\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ajQF6fX0igt2",
        "outputId": "aa35fed4-7d42-4e71-b6f1-1c8ce878d8bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m You are an expert in Robotics software developing. Explain the drone application in healthcare industry.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'Robot': 'drone', 'industry': 'healthcare', 'text': \"Drones have a wide range of applications in the healthcare industry, including but not limited to:\\n\\n1. Medical supply delivery: Drones can be used to deliver medical supplies such as medications, blood samples, vaccines, and emergency equipment to remote or hard-to-reach areas. This can be especially useful in disaster relief situations or in rural areas where access to medical facilities is limited.\\n\\n2. Telemedicine support: Drones equipped with cameras and other sensors can be used to provide telemedicine support to patients in remote areas. They can transmit real-time video and other data to healthcare providers, allowing them to assess the patient's condition and provide guidance on treatment.\\n\\n3. Search and rescue operations: Drones can be used to search for and locate missing persons in emergency situations. They can cover large areas quickly and efficiently, providing valuable information to search and rescue teams.\\n\\n4. Monitoring and surveillance: Drones can be used to monitor and surveil large areas, such as hospital grounds or public spaces, to ensure the safety and security of patients, staff, and visitors.\\n\\n5. Disaster response: Drones can be deployed in disaster response situations to assess damage, locate survivors, and provide critical information to emergency responders. They can also be used to deliver emergency supplies and medical aid to affected areas.\\n\\nOverall, drones have the potential to revolutionize the healthcare industry by improving access to care, enhancing patient outcomes, and increasing efficiency in healthcare delivery. As an expert in robotics software development, you can play a key role in developing and implementing drone technology in healthcare applications.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = 'What is the best food of {country}? Name five most to see places in it. use bulletpoints format.'\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = LLMChain(\n",
        "\n",
        "                 llm = llm,\n",
        "                 prompt = prompt_template,\n",
        "                 verbose = True\n",
        "\n",
        ")\n",
        "\n",
        "country = input('country')\n",
        "output = chain.invoke(country)\n",
        "print(output['text'])"
      ],
      "metadata": {
        "id": "c46SLbmhlSwn",
        "outputId": "d07a6bea-d560-4b89-9de0-269181d83788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "countryUK\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWhat is the best food of UK? Name five most to see places in it. use bulletpoints format.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The best food of the UK is subjective and can vary depending on personal preferences. However, some popular dishes include:\n",
            "\n",
            "1. Fish and chips\n",
            "2. Roast beef and Yorkshire pudding\n",
            "3. Full English breakfast\n",
            "4. Shepherd's pie\n",
            "5. Chicken tikka masala\n",
            "\n",
            "Five most must-see places in the UK:\n",
            "\n",
            "- London (including Big Ben, Buckingham Palace, and the Tower of London)\n",
            "- Edinburgh (including Edinburgh Castle and the Royal Mile)\n",
            "- Stonehenge\n",
            "- The Lake District\n",
            "- The Cotswolds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain Agents in Action: Python REPL"
      ],
      "metadata": {
        "id": "t8wpUNar5ztD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_experimental"
      ],
      "metadata": {
        "id": "ssHLvV4P5o8z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "# REPL is an acronym for Read, Evaluate, Print, and Loop.\n",
        "# Developers use REPL Python to communicate with the Python Interpreter.\n",
        "# In contrast to running a Python file, you can input commands in the REPL and see the results displayed immediately.\n",
        "\n",
        "python_repl = PythonREPL()\n",
        "python_repl.run('print([n for n in range (1,100) if n%13==0])')"
      ],
      "metadata": {
        "id": "TdLdivy_7y1d",
        "outputId": "9ddc61a9-7d30-4fc0-c0e4-5810d5530488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[13, 26, 39, 52, 65, 78, 91]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model = 'gpt-4-turbo-preview', temperature=0)\n",
        "agent_executer = create_python_agent(\n",
        "    llm = llm,\n",
        "    tool = PythonREPLTool(),\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "\n",
        "agent_executer.invoke('calculate 5.1**7.1')\n",
        "\n",
        "print ('------------------')\n",
        "\n",
        "agent_executer.invoke('calculate the square root of factorial of 12 and display it with 5 decimal points')"
      ],
      "metadata": {
        "id": "kIwv1NtzKVmt",
        "outputId": "5820fd4f-99f5-4ac4-966e-964c0182d293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo calculate \\(5.1^{7.1}\\), I will use Python's exponentiation operator `**`.\n",
            "Action: Python_REPL\n",
            "Action Input: print(5.1**7.1)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m105620.50759235727\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 105620.50759235727\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo solve this, I will first calculate the factorial of 12 using the `math` module's `factorial` function. Then, I will calculate the square root of that result using the `sqrt` function from the same module. Finally, I will format the result to display it with 5 decimal points using the `format` function.\n",
            "Action: Python_REPL\n",
            "Action Input: import math\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow that the math module is imported, I can calculate the factorial of 12.\n",
            "Action: Python_REPL\n",
            "Action Input: factorial_12 = math.factorial(12)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow that I have calculated the factorial of 12, I will calculate its square root.\n",
            "Action: Python_REPL\n",
            "Action Input: sqrt_factorial_12 = math.sqrt(factorial_12)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow that I have the square root of the factorial of 12, I will format it to display with 5 decimal points.\n",
            "Action: Python_REPL\n",
            "Action Input: print(f\"{sqrt_factorial_12:.5f}\")\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m21886.10518\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 21886.10518\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'calculate the square root of factorial of 12 and display it with 5 decimal points',\n",
              " 'output': '21886.10518'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}