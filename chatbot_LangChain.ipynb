{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwRYgSngp/WXEKg5/4Mzz+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R0b0t-Maker/LLM-T/blob/main/chatbot_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3IA2GjvP77S",
        "outputId": "5e50d3ca-9c29-4d6b-acb7-d6631daf2fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/drive/MyDrive/Udemy/LangChain/p-1/requirements.txt\" -q"
      ],
      "metadata": {
        "id": "29i1DIhZXXaQ",
        "outputId": "c6e6483c-e9b9-4d9c-e4b4-54a430b31ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip show openai"
      ],
      "metadata": {
        "id": "V0l5iguyYiiC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet openai python-dotenv\n"
      ],
      "metadata": {
        "id": "O48scuZCmsqW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(\"/content/drive/MyDrive/Udemy/LangChain/p-1/.env\", override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR_9JiDskM6Q",
        "outputId": "c66c65c6-6584-4de3-a960-976c0d0f78e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "VkId88TFtCYx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm= ChatOpenAI()\n",
        "llm.invoke('What is AI model?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpPkygxOtKyp",
        "outputId": "c499175a-2520-4e3d-ba3d-5626c653c7c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='An AI model is a mathematical algorithm that is used to make predictions or decisions based on data. It is created through the training of a machine learning system on a specific dataset, which allows it to recognize patterns and relationships within the data. AI models are used in various applications such as image recognition, natural language processing, and recommendation systems.', response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 12, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7b8ee5fc-8645-4233-ac22-944d5c135651-0')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke('What is AI model?')\n",
        "print(output.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UXL81b4uFBx",
        "outputId": "a1250954-03db-4e3a-8553-c6ec1671df49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An AI model is a mathematical algorithm or computational model that is used by artificial intelligence systems to make predictions, decisions, or perform specific tasks. AI models are trained using large amounts of data and are designed to learn from patterns and information in the data to make accurate predictions or decisions. These models can be used for a wide range of applications, such as image recognition, natural language processing, and predictive analytics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(ChatOpenAI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwFvIIlVuulR",
        "outputId": "816691e5-e1ce-461c-9279-3f32b39abf8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class ChatOpenAI in module langchain_openai.chat_models.base:\n",
            "\n",
            "class ChatOpenAI(BaseChatOpenAI)\n",
            " |  ChatOpenAI(*, name: Optional[str] = None, cache: ForwardRef('Union[BaseCache, bool, None]') = None, verbose: bool = None, callbacks: ForwardRef('Callbacks') = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, custom_get_token_ids: Optional[Callable[[str], List[int]]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, client: Any = None, async_client: Any = None, model: str = 'gpt-3.5-turbo', temperature: float = 0.7, model_kwargs: Dict[str, Any] = None, api_key: Optional[pydantic.v1.types.SecretStr] = None, base_url: Optional[str] = None, organization: Optional[str] = None, openai_proxy: Optional[str] = None, timeout: Union[float, Tuple[float, float], Any, NoneType] = None, max_retries: int = 2, streaming: bool = False, n: int = 1, max_tokens: Optional[int] = None, tiktoken_model_name: Optional[str] = None, default_headers: Optional[Mapping[str, str]] = None, default_query: Optional[Mapping[str, object]] = None, http_client: Optional[Any] = None, http_async_client: Optional[Any] = None) -> None\n",
            " |  \n",
            " |  `OpenAI` Chat large language models API.\n",
            " |  \n",
            " |  To use, you should have the environment variable ``OPENAI_API_KEY``\n",
            " |  set with your API key, or pass it as a named parameter to the constructor.\n",
            " |  \n",
            " |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
            " |  in, even if not explicitly saved on this class.\n",
            " |  \n",
            " |  Example:\n",
            " |      .. code-block:: python\n",
            " |  \n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |  \n",
            " |          model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      ChatOpenAI\n",
            " |      BaseChatOpenAI\n",
            " |      langchain_core.language_models.chat_models.BaseChatModel\n",
            " |      langchain_core.language_models.base.BaseLanguageModel\n",
            " |      langchain_core.runnables.base.RunnableSerializable\n",
            " |      langchain_core.load.serializable.Serializable\n",
            " |      pydantic.v1.main.BaseModel\n",
            " |      pydantic.v1.utils.Representation\n",
            " |      langchain_core.runnables.base.Runnable\n",
            " |      typing.Generic\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  get_lc_namespace() -> 'List[str]' from pydantic.v1.main.ModelMetaclass\n",
            " |      Get the namespace of the langchain object.\n",
            " |  \n",
            " |  is_lc_serializable() -> 'bool' from pydantic.v1.main.ModelMetaclass\n",
            " |      Return whether this model can be serialized by Langchain.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  lc_attributes\n",
            " |      List of attribute names that should be included in the serialized kwargs.\n",
            " |      \n",
            " |      These attributes must be accepted by the constructor.\n",
            " |  \n",
            " |  lc_secrets\n",
            " |      A map of constructor argument names to secret ids.\n",
            " |      \n",
            " |      For example,\n",
            " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  __class_vars__ = set()\n",
            " |  \n",
            " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
            " |  \n",
            " |  __custom_root_type__ = False\n",
            " |  \n",
            " |  __exclude_fields__ = {'async_client': True, 'callback_manager': True, ...\n",
            " |  \n",
            " |  __fields__ = {'async_client': ModelField(name='async_client', type=Opt...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __include_fields__ = None\n",
            " |  \n",
            " |  __parameters__ = ()\n",
            " |  \n",
            " |  __post_root_validators__ = [(False, <function BaseChatModel.raise_depr...\n",
            " |  \n",
            " |  __pre_root_validators__ = [<function BaseChatOpenAI.build_extra>]\n",
            " |  \n",
            " |  __private_attributes__ = {}\n",
            " |  \n",
            " |  __schema_cache__ = {}\n",
            " |  \n",
            " |  __signature__ = <Signature (*, name: Optional[str] = None, cache...htt...\n",
            " |  \n",
            " |  __validators__ = {'verbose': [<pydantic.v1.class_validators.Validator ...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseChatOpenAI:\n",
            " |  \n",
            " |  bind_functions(self, functions: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', function_call: \"Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
            " |      Bind functions (and other objects) to this chat model.\n",
            " |      \n",
            " |      Assumes model is compatible with OpenAI function-calling API.\n",
            " |      \n",
            " |      NOTE: Using bind_tools is recommended instead, as the `functions` and\n",
            " |          `function_call` request parameters are officially marked as deprecated by\n",
            " |          OpenAI.\n",
            " |      \n",
            " |      Args:\n",
            " |          functions: A list of function definitions to bind to this chat model.\n",
            " |              Can be  a dictionary, pydantic model, or callable. Pydantic\n",
            " |              models and callables will be automatically converted to\n",
            " |              their schema dictionary representation.\n",
            " |          function_call: Which function to require the model to call.\n",
            " |              Must be the name of the single provided function or\n",
            " |              \"auto\" to automatically determine which function to call\n",
            " |              (if any).\n",
            " |          **kwargs: Any additional parameters to pass to the\n",
            " |              :class:`~langchain.runnable.Runnable` constructor.\n",
            " |  \n",
            " |  bind_tools(self, tools: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', *, tool_choice: \"Optional[Union[dict, str, Literal['auto', 'none', 'required', 'any'], bool]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
            " |      Bind tool-like objects to this chat model.\n",
            " |      \n",
            " |      Assumes model is compatible with OpenAI tool-calling API.\n",
            " |      \n",
            " |      Args:\n",
            " |          tools: A list of tool definitions to bind to this chat model.\n",
            " |              Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
            " |              models, callables, and BaseTools will be automatically converted to\n",
            " |              their schema dictionary representation.\n",
            " |          tool_choice: Which tool to require the model to call.\n",
            " |              Options are:\n",
            " |              name of the tool (str): calls corresponding tool;\n",
            " |              \"auto\": automatically selects a tool (including no tool);\n",
            " |              \"none\": does not call a tool;\n",
            " |              \"any\" or \"required\": force at least one tool to be called;\n",
            " |              True: forces tool call (requires `tools` be length 1);\n",
            " |              False: no effect;\n",
            " |      \n",
            " |              or a dict of the form:\n",
            " |              {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
            " |          **kwargs: Any additional parameters to pass to the\n",
            " |              :class:`~langchain.runnable.Runnable` constructor.\n",
            " |  \n",
            " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
            " |      Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n",
            " |      \n",
            " |      Official documentation: https://github.com/openai/openai-cookbook/blob/\n",
            " |      main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
            " |  \n",
            " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
            " |      Get the tokens present in the text with tiktoken package.\n",
            " |  \n",
            " |  with_structured_output(self, schema: 'Optional[_DictOrPydanticClass]' = None, *, method: \"Literal['function_calling', 'json_mode']\" = 'function_calling', include_raw: 'bool' = False, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, _DictOrPydantic]'\n",
            " |      Model wrapper that returns outputs formatted to match the given schema.\n",
            " |      \n",
            " |              Args:\n",
            " |                  schema: The output schema as a dict or a Pydantic class. If a Pydantic class\n",
            " |                      then the model output will be an object of that class. If a dict then\n",
            " |                      the model output will be a dict. With a Pydantic class the returned\n",
            " |                      attributes will be validated, whereas with a dict they will not be. If\n",
            " |                      `method` is \"function_calling\" and `schema` is a dict, then the dict\n",
            " |                      must match the OpenAI function-calling spec or be a valid JSON schema\n",
            " |                      with top level 'title' and 'description' keys specified.\n",
            " |                  method: The method for steering model generation, either \"function_calling\"\n",
            " |                      or \"json_mode\". If \"function_calling\" then the schema will be converted\n",
            " |                      to an OpenAI function and the returned model will make use of the\n",
            " |                      function-calling API. If \"json_mode\" then OpenAI's JSON mode will be\n",
            " |                      used. Note that if using \"json_mode\" then you must include instructions\n",
            " |                      for formatting the output into the desired schema into the model call.\n",
            " |                  include_raw: If False then only the parsed structured output is returned. If\n",
            " |                      an error occurs during model output parsing it will be raised. If True\n",
            " |                      then both the raw model response (a BaseMessage) and the parsed model\n",
            " |                      response will be returned. If an error occurs during output parsing it\n",
            " |                      will be caught and returned as well. The final output is always a dict\n",
            " |                      with keys \"raw\", \"parsed\", and \"parsing_error\".\n",
            " |      \n",
            " |              Returns:\n",
            " |                  A Runnable that takes any ChatModel input and returns as output:\n",
            " |      \n",
            " |                      If include_raw is True then a dict with keys:\n",
            " |                          raw: BaseMessage\n",
            " |                          parsed: Optional[_DictOrPydantic]\n",
            " |                          parsing_error: Optional[BaseException]\n",
            " |      \n",
            " |                      If include_raw is False then just _DictOrPydantic is returned,\n",
            " |                      where _DictOrPydantic depends on the schema:\n",
            " |      \n",
            " |                      If schema is a Pydantic class then _DictOrPydantic is the Pydantic\n",
            " |                          class.\n",
            " |      \n",
            " |                      If schema is a dict then _DictOrPydantic is a dict.\n",
            " |      \n",
            " |              Example: Function-calling, Pydantic schema (method=\"function_calling\", include_raw=False):\n",
            " |                  .. code-block:: python\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |                      from langchain_core.pydantic_v1 import BaseModel\n",
            " |      \n",
            " |                      class AnswerWithJustification(BaseModel):\n",
            " |                          '''An answer to the user question along with justification for the answer.'''\n",
            " |                          answer: str\n",
            " |                          justification: str\n",
            " |      \n",
            " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
            " |                      structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
            " |      \n",
            " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
            " |      \n",
            " |                      # -> AnswerWithJustification(\n",
            " |                      #     answer='They weigh the same',\n",
            " |                      #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
            " |                      # )\n",
            " |      \n",
            " |              Example: Function-calling, Pydantic schema (method=\"function_calling\", include_raw=True):\n",
            " |                  .. code-block:: python\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |                      from langchain_core.pydantic_v1 import BaseModel\n",
            " |      \n",
            " |                      class AnswerWithJustification(BaseModel):\n",
            " |                          '''An answer to the user question along with justification for the answer.'''\n",
            " |                          answer: str\n",
            " |                          justification: str\n",
            " |      \n",
            " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
            " |                      structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n",
            " |      \n",
            " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
            " |                      # -> {\n",
            " |                      #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n",
            " |                      #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n",
            " |                      #     'parsing_error': None\n",
            " |                      # }\n",
            " |      \n",
            " |              Example: Function-calling, dict schema (method=\"function_calling\", include_raw=False):\n",
            " |                  .. code-block:: python\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |                      from langchain_core.pydantic_v1 import BaseModel\n",
            " |                      from langchain_core.utils.function_calling import convert_to_openai_tool\n",
            " |      \n",
            " |                      class AnswerWithJustification(BaseModel):\n",
            " |                          '''An answer to the user question along with justification for the answer.'''\n",
            " |                          answer: str\n",
            " |                          justification: str\n",
            " |      \n",
            " |                      dict_schema = convert_to_openai_tool(AnswerWithJustification)\n",
            " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
            " |                      structured_llm = llm.with_structured_output(dict_schema)\n",
            " |      \n",
            " |                      structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n",
            " |                      # -> {\n",
            " |                      #     'answer': 'They weigh the same',\n",
            " |                      #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
            " |                      # }\n",
            " |      \n",
            " |              Example: JSON mode, Pydantic schema (method=\"json_mode\", include_raw=True):\n",
            " |                  .. code-block::\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |                      from langchain_core.pydantic_v1 import BaseModel\n",
            " |      \n",
            " |                      class AnswerWithJustification(BaseModel):\n",
            " |                          answer: str\n",
            " |                          justification: str\n",
            " |      \n",
            " |                      llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
            " |                      structured_llm = llm.with_structured_output(\n",
            " |                          AnswerWithJustification,\n",
            " |                          method=\"json_mode\",\n",
            " |                          include_raw=True\n",
            " |                      )\n",
            " |      \n",
            " |                      structured_llm.invoke(\n",
            " |                          \"Answer the following question. \"\n",
            " |                          \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\n",
            " |      \n",
            " |      \"\n",
            " |                          \"What's heavier a pound of bricks or a pound of feathers?\"\n",
            " |                      )\n",
            " |                      # -> {\n",
            " |                      #     'raw': AIMessage(content='{\n",
            " |          \"answer\": \"They are both the same weight.\",\n",
            " |          \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \n",
            " |      }'),\n",
            " |                      #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n",
            " |                      #     'parsing_error': None\n",
            " |                      # }\n",
            " |      \n",
            " |              Example: JSON mode, no schema (schema=None, method=\"json_mode\", include_raw=True):\n",
            " |                  .. code-block::\n",
            " |      \n",
            " |                      from langchain_openai import ChatOpenAI\n",
            " |      \n",
            " |                      structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n",
            " |      \n",
            " |                      structured_llm.invoke(\n",
            " |                          \"Answer the following question. \"\n",
            " |                          \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\n",
            " |      \n",
            " |      \"\n",
            " |                          \"What's heavier a pound of bricks or a pound of feathers?\"\n",
            " |                      )\n",
            " |                      # -> {\n",
            " |                      #     'raw': AIMessage(content='{\n",
            " |          \"answer\": \"They are both the same weight.\",\n",
            " |          \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \n",
            " |      }'),\n",
            " |                      #     'parsed': {\n",
            " |                      #         'answer': 'They are both the same weight.',\n",
            " |                      #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n",
            " |                      #     },\n",
            " |                      #     'parsing_error': None\n",
            " |                      # }\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from BaseChatOpenAI:\n",
            " |  \n",
            " |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.v1.main.ModelMetaclass\n",
            " |      Build extra kwargs from additional params that were passed in.\n",
            " |  \n",
            " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
            " |      Validate that api key and python package exists in environment.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from BaseChatOpenAI:\n",
            " |  \n",
            " |  Config = <class 'langchain_openai.chat_models.base.BaseChatOpenAI.Conf...\n",
            " |      Configuration for this pydantic object.\n",
            " |  \n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |  \n",
            " |  __call__(self, messages: 'List[BaseMessage]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use invoke instead.\n",
            " |  \n",
            " |  async agenerate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          messages: List of list of messages.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts and return model generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
            " |              converted to match the format of any language model (string for pure\n",
            " |              text generation models and BaseMessages for chat models).\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      Default implementation of ainvoke, calls invoke from a thread.\n",
            " |      \n",
            " |      The default implementation allows usage of async code even if\n",
            " |      the runnable did not implement a native async version of invoke.\n",
            " |      \n",
            " |      Subclasses should override this method if they can run asynchronously.\n",
            " |  \n",
            " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use ainvoke instead.\n",
            " |  \n",
            " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use ainvoke instead.\n",
            " |  \n",
            " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
            " |      Default implementation of astream, which calls ainvoke.\n",
            " |      Subclasses should override this method if they support streaming output.\n",
            " |  \n",
            " |  call_as_llm(self, message: 'str', stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use invoke instead.\n",
            " |  \n",
            " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
            " |      Return a dictionary of the LLM.\n",
            " |  \n",
            " |  generate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          messages: List of list of messages.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |      \n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |      \n",
            " |      Use this method when you want to:\n",
            " |          1. take advantage of batched calls,\n",
            " |          2. need more output from the model than just the top generated value,\n",
            " |          3. are building chains that are agnostic to the underlying language model\n",
            " |              type (e.g., pure text completion models vs chat models).\n",
            " |      \n",
            " |      Args:\n",
            " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
            " |              converted to match the format of any language model (string for pure\n",
            " |              text generation models and BaseMessages for chat models).\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |              prompt and additional model provider-specific output.\n",
            " |  \n",
            " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      Transform a single input into an output. Override to implement.\n",
            " |      \n",
            " |      Args:\n",
            " |          input: The input to the runnable.\n",
            " |          config: A config to use when invoking the runnable.\n",
            " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
            " |             purposes, 'max_concurrency' for controlling how much work to do\n",
            " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
            " |             for more details.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The output of the runnable.\n",
            " |  \n",
            " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use invoke instead.\n",
            " |  \n",
            " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      [*Deprecated*] \n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. deprecated:: langchain-core==0.1.7\n",
            " |         Use invoke instead.\n",
            " |  \n",
            " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
            " |      Default implementation of stream, which calls invoke.\n",
            " |      Subclasses should override this method if they support streaming output.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |  \n",
            " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
            " |      Raise deprecation warning if callback_manager is used.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |  \n",
            " |  OutputType\n",
            " |      Get the output type for this runnable.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |  \n",
            " |  __orig_bases__ = (langchain_core.language_models.base.BaseLanguageMode...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |  \n",
            " |  get_num_tokens(self, text: 'str') -> 'int'\n",
            " |      Get the number of tokens present in the text.\n",
            " |      \n",
            " |      Useful for checking if an input will fit in a model's context window.\n",
            " |      \n",
            " |      Args:\n",
            " |          text: The string input to tokenize.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The integer number of tokens in the text.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |  \n",
            " |  set_verbose(verbose: 'Optional[bool]') -> 'bool' from pydantic.v1.main.ModelMetaclass\n",
            " |      If verbose is None, set it.\n",
            " |      \n",
            " |      This allows users to pass in None as verbose to access the global setting.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |  \n",
            " |  InputType\n",
            " |      Get the input type for this runnable.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
            " |  \n",
            " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
            " |      Configure alternatives for runnables that can be set at runtime.\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_anthropic import ChatAnthropic\n",
            " |          from langchain_core.runnables.utils import ConfigurableField\n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |      \n",
            " |          model = ChatAnthropic(\n",
            " |              model_name=\"claude-3-sonnet-20240229\"\n",
            " |          ).configurable_alternatives(\n",
            " |              ConfigurableField(id=\"llm\"),\n",
            " |              default_key=\"anthropic\",\n",
            " |              openai=ChatOpenAI()\n",
            " |          )\n",
            " |      \n",
            " |          # uses the default model ChatAnthropic\n",
            " |          print(model.invoke(\"which organization created you?\").content)\n",
            " |      \n",
            " |          # uses ChatOpenaAI\n",
            " |          print(\n",
            " |              model.with_config(\n",
            " |                  configurable={\"llm\": \"openai\"}\n",
            " |              ).invoke(\"which organization created you?\").content\n",
            " |          )\n",
            " |  \n",
            " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
            " |      Configure particular runnable fields at runtime.\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_core.runnables import ConfigurableField\n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |      \n",
            " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
            " |              max_tokens=ConfigurableField(\n",
            " |                  id=\"output_token_number\",\n",
            " |                  name=\"Max tokens in the output\",\n",
            " |                  description=\"The maximum number of tokens in the output\",\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |          # max_tokens = 20\n",
            " |          print(\n",
            " |              \"max_tokens_20: \",\n",
            " |              model.invoke(\"tell me something about chess\").content\n",
            " |          )\n",
            " |      \n",
            " |          # max_tokens = 200\n",
            " |          print(\"max_tokens_200: \", model.with_config(\n",
            " |              configurable={\"output_token_number\": 200}\n",
            " |              ).invoke(\"tell me something about chess\").content\n",
            " |          )\n",
            " |  \n",
            " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
            " |      Serialize the runnable to JSON.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
            " |  \n",
            " |  __repr_args__(self) -> Any\n",
            " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
            " |      \n",
            " |      Can either return:\n",
            " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
            " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
            " |  \n",
            " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
            " |  \n",
            " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
            " |      A unique identifier for this class for serialization purposes.\n",
            " |      \n",
            " |      The unique identifier is a list of strings that describes the path\n",
            " |      to the object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __getstate__(self) -> 'DictAny'\n",
            " |  \n",
            " |  __init__(__pydantic_self__, **data: Any) -> None\n",
            " |      Create a new model by parsing and validating input data from keyword arguments.\n",
            " |      \n",
            " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
            " |  \n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      so `dict(model)` works\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state: 'DictAny') -> None\n",
            " |  \n",
            " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
            " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
            " |      \n",
            " |      :param include: fields to include in new model\n",
            " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
            " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
            " |          the new model: you should trust this data\n",
            " |      :param deep: set to `True` to make a deep copy of the model\n",
            " |      :return: new model instance\n",
            " |  \n",
            " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
            " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
            " |      \n",
            " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
            " |      Same as update_forward_refs but will not raise exception\n",
            " |      when forward references are not defined.\n",
            " |  \n",
            " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
            " |  \n",
            " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
            " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
            " |  \n",
            " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __fields_set__\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.v1.utils.Representation:\n",
            " |  \n",
            " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __repr_name__(self) -> str\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |  \n",
            " |  __repr_str__(self, join_str: str) -> str\n",
            " |  \n",
            " |  __rich_repr__(self) -> 'RichReprResult'\n",
            " |      Get fields for Rich library\n",
            " |  \n",
            " |  __str__(self) -> str\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
            " |  \n",
            " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
            " |      Compose this runnable with another object to create a RunnableSequence.\n",
            " |  \n",
            " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
            " |      Compose this runnable with another object to create a RunnableSequence.\n",
            " |  \n",
            " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
            " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
            " |      \n",
            " |      The default implementation of batch works well for IO bound runnables.\n",
            " |      \n",
            " |      Subclasses should override this method if they can batch more efficiently;\n",
            " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
            " |  \n",
            " |  async abatch_as_completed(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Tuple[int, Union[Output, Exception]]]'\n",
            " |      Run ainvoke in parallel on a list of inputs,\n",
            " |      yielding results as they complete.\n",
            " |  \n",
            " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
            " |      Assigns new fields to the dict output of this runnable.\n",
            " |      Returns a new runnable.\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
            " |          from langchain_core.output_parsers import StrOutputParser\n",
            " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
            " |          from langchain_core.runnables import Runnable\n",
            " |          from operator import itemgetter\n",
            " |      \n",
            " |          prompt = (\n",
            " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
            " |              + \"{question}\"\n",
            " |          )\n",
            " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
            " |      \n",
            " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
            " |      \n",
            " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
            " |      \n",
            " |          print(chain_with_assign.input_schema.schema())\n",
            " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
            " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
            " |          print(chain_with_assign.output_schema.schema()) #\n",
            " |          {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
            " |          {'str': {'title': 'Str',\n",
            " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
            " |  \n",
            " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
            " |      [*Beta*] Generate a stream of events.\n",
            " |      \n",
            " |      Use to create an iterator over StreamEvents that provide real-time information\n",
            " |      about the progress of the runnable, including StreamEvents from intermediate\n",
            " |      results.\n",
            " |      \n",
            " |      A StreamEvent is a dictionary with the following schema:\n",
            " |      \n",
            " |      - ``event``: **str** - Event names are of the\n",
            " |          format: on_[runnable_type]_(start|stream|end).\n",
            " |      - ``name``: **str** - The name of the runnable that generated the event.\n",
            " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
            " |          the runnable that emitted the event.\n",
            " |          A child runnable that gets invoked as part of the execution of a\n",
            " |          parent runnable is assigned its own unique ID.\n",
            " |      - ``tags``: **Optional[List[str]]** - The tags of the runnable that generated\n",
            " |          the event.\n",
            " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the runnable\n",
            " |          that generated the event.\n",
            " |      - ``data``: **Dict[str, Any]**\n",
            " |      \n",
            " |      \n",
            " |      Below is a table that illustrates some evens that might be emitted by various\n",
            " |      chains. Metadata fields have been omitted from the table for brevity.\n",
            " |      Chain definitions have been included after the table.\n",
            " |      \n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
            " |      +======================+==================+=================================+===============================================+=================================================+\n",
            " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
            " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
            " |      \n",
            " |      Here are declarations associated with the events shown above:\n",
            " |      \n",
            " |      `format_docs`:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          def format_docs(docs: List[Document]) -> str:\n",
            " |              '''Format the docs.'''\n",
            " |              return \", \".join([doc.page_content for doc in docs])\n",
            " |      \n",
            " |          format_docs = RunnableLambda(format_docs)\n",
            " |      \n",
            " |      `some_tool`:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          @tool\n",
            " |          def some_tool(x: int, y: str) -> dict:\n",
            " |              '''Some_tool.'''\n",
            " |              return {\"x\": x, \"y\": y}\n",
            " |      \n",
            " |      `prompt`:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          template = ChatPromptTemplate.from_messages(\n",
            " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
            " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
            " |      \n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |      \n",
            " |          async def reverse(s: str) -> str:\n",
            " |              return s[::-1]\n",
            " |      \n",
            " |          chain = RunnableLambda(func=reverse)\n",
            " |      \n",
            " |          events = [\n",
            " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
            " |          ]\n",
            " |      \n",
            " |          # will produce the following events (run_id has been omitted for brevity):\n",
            " |          [\n",
            " |              {\n",
            " |                  \"data\": {\"input\": \"hello\"},\n",
            " |                  \"event\": \"on_chain_start\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |              {\n",
            " |                  \"data\": {\"chunk\": \"olleh\"},\n",
            " |                  \"event\": \"on_chain_stream\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |              {\n",
            " |                  \"data\": {\"output\": \"olleh\"},\n",
            " |                  \"event\": \"on_chain_end\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |          ]\n",
            " |      \n",
            " |      Args:\n",
            " |          input: The input to the runnable.\n",
            " |          config: The config to use for the runnable.\n",
            " |          version: The version of the schema to use.\n",
            " |                   Currently only version 1 is available.\n",
            " |                   No default will be assigned until the API is stabilized.\n",
            " |          include_names: Only include events from runnables with matching names.\n",
            " |          include_types: Only include events from runnables with matching types.\n",
            " |          include_tags: Only include events from runnables with matching tags.\n",
            " |          exclude_names: Exclude events from runnables with matching names.\n",
            " |          exclude_types: Exclude events from runnables with matching types.\n",
            " |          exclude_tags: Exclude events from runnables with matching tags.\n",
            " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
            " |              These will be passed to astream_log as this implementation\n",
            " |              of astream_events is built on top of astream_log.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An async stream of StreamEvents.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      .. beta::\n",
            " |         This API is in beta and may change in the future.\n",
            " |  \n",
            " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
            " |      Stream all output from a runnable, as reported to the callback system.\n",
            " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
            " |      \n",
            " |      Output is streamed as Log objects, which include a list of\n",
            " |      jsonpatch ops that describe how the state of the run has changed in each\n",
            " |      step, and the final state of the run.\n",
            " |      \n",
            " |      The jsonpatch ops can be applied in order to construct state.\n",
            " |      \n",
            " |      Args:\n",
            " |          input: The input to the runnable.\n",
            " |          config: The config to use for the runnable.\n",
            " |          diff: Whether to yield diffs between each step, or the current state.\n",
            " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
            " |          include_names: Only include logs with these names.\n",
            " |          include_types: Only include logs with these types.\n",
            " |          include_tags: Only include logs with these tags.\n",
            " |          exclude_names: Exclude logs with these names.\n",
            " |          exclude_types: Exclude logs with these types.\n",
            " |          exclude_tags: Exclude logs with these tags.\n",
            " |  \n",
            " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
            " |      Default implementation of atransform, which buffers input and calls astream.\n",
            " |      Subclasses should override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |  \n",
            " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
            " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
            " |      \n",
            " |      The default implementation of batch works well for IO bound runnables.\n",
            " |      \n",
            " |      Subclasses should override this method if they can batch more efficiently;\n",
            " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
            " |  \n",
            " |  batch_as_completed(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[Tuple[int, Union[Output, Exception]]]'\n",
            " |      Run invoke in parallel on a list of inputs,\n",
            " |      yielding results as they complete.\n",
            " |  \n",
            " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind arguments to a Runnable, returning a new Runnable.\n",
            " |      \n",
            " |      Useful when a runnable in a chain requires an argument that is not\n",
            " |      in the output of the previous runnable or included in the user input.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          from langchain_community.chat_models import ChatOllama\n",
            " |          from langchain_core.output_parsers import StrOutputParser\n",
            " |      \n",
            " |          llm = ChatOllama(model='llama2')\n",
            " |      \n",
            " |          # Without bind.\n",
            " |          chain = (\n",
            " |              llm\n",
            " |              | StrOutputParser()\n",
            " |          )\n",
            " |      \n",
            " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
            " |          # Output is 'One two three four five.'\n",
            " |      \n",
            " |          # With bind.\n",
            " |          chain = (\n",
            " |              llm.bind(stop=[\"three\"])\n",
            " |              | StrOutputParser()\n",
            " |          )\n",
            " |      \n",
            " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
            " |          # Output is 'One two'\n",
            " |  \n",
            " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
            " |      The type of config this runnable accepts specified as a pydantic model.\n",
            " |      \n",
            " |      To mark a field as configurable, see the `configurable_fields`\n",
            " |      and `configurable_alternatives` methods.\n",
            " |      \n",
            " |      Args:\n",
            " |          include: A list of fields to include in the config schema.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate config.\n",
            " |  \n",
            " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
            " |      Return a graph representation of this runnable.\n",
            " |  \n",
            " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
            " |      Get a pydantic model that can be used to validate input to the runnable.\n",
            " |      \n",
            " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
            " |      methods will have a dynamic input schema that depends on which\n",
            " |      configuration the runnable is invoked with.\n",
            " |      \n",
            " |      This method allows to get an input schema for a specific configuration.\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate input.\n",
            " |  \n",
            " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
            " |      Get the name of the runnable.\n",
            " |  \n",
            " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
            " |      Get a pydantic model that can be used to validate output to the runnable.\n",
            " |      \n",
            " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
            " |      methods will have a dynamic output schema that depends on which\n",
            " |      configuration the runnable is invoked with.\n",
            " |      \n",
            " |      This method allows to get an output schema for a specific configuration.\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate output.\n",
            " |  \n",
            " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
            " |  \n",
            " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
            " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
            " |      by calling invoke() with each input.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |                  from langchain_core.runnables import RunnableLambda\n",
            " |      \n",
            " |                  def _lambda(x: int) -> int:\n",
            " |                      return x + 1\n",
            " |      \n",
            " |                  runnable = RunnableLambda(_lambda)\n",
            " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
            " |  \n",
            " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
            " |      Pick keys from the dict output of this runnable.\n",
            " |      \n",
            " |      Pick single key:\n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |              import json\n",
            " |      \n",
            " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
            " |      \n",
            " |              as_str = RunnableLambda(str)\n",
            " |              as_json = RunnableLambda(json.loads)\n",
            " |              chain = RunnableMap(str=as_str, json=as_json)\n",
            " |      \n",
            " |              chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
            " |      \n",
            " |              json_only_chain = chain.pick(\"json\")\n",
            " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> [1, 2, 3]\n",
            " |      \n",
            " |      Pick list of keys:\n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |              from typing import Any\n",
            " |      \n",
            " |              import json\n",
            " |      \n",
            " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
            " |      \n",
            " |              as_str = RunnableLambda(str)\n",
            " |              as_json = RunnableLambda(json.loads)\n",
            " |              def as_bytes(x: Any) -> bytes:\n",
            " |                  return bytes(x, \"utf-8\")\n",
            " |      \n",
            " |              chain = RunnableMap(\n",
            " |                  str=as_str,\n",
            " |                  json=as_json,\n",
            " |                  bytes=RunnableLambda(as_bytes)\n",
            " |              )\n",
            " |      \n",
            " |              chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
            " |      \n",
            " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
            " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
            " |  \n",
            " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
            " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
            " |      \n",
            " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
            " |      \n",
            " |      Example:\n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |              from langchain_core.runnables import RunnableLambda\n",
            " |      \n",
            " |              def add_one(x: int) -> int:\n",
            " |                  return x + 1\n",
            " |      \n",
            " |              def mul_two(x: int) -> int:\n",
            " |                  return x * 2\n",
            " |      \n",
            " |              runnable_1 = RunnableLambda(add_one)\n",
            " |              runnable_2 = RunnableLambda(mul_two)\n",
            " |              sequence = runnable_1.pipe(runnable_2)\n",
            " |              # Or equivalently:\n",
            " |              # sequence = runnable_1 | runnable_2\n",
            " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
            " |              sequence.invoke(1)\n",
            " |              await sequence.ainvoke(1)\n",
            " |              # -> 4\n",
            " |      \n",
            " |              sequence.batch([1, 2, 3])\n",
            " |              await sequence.abatch([1, 2, 3])\n",
            " |              # -> [4, 6, 8]\n",
            " |  \n",
            " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
            " |      Default implementation of transform, which buffers input and then calls stream.\n",
            " |      Subclasses should override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |  \n",
            " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind config to a Runnable, returning a new Runnable.\n",
            " |  \n",
            " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
            " |      Add fallbacks to a runnable, returning a new Runnable.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |          .. code-block:: python\n",
            " |      \n",
            " |              from typing import Iterator\n",
            " |      \n",
            " |              from langchain_core.runnables import RunnableGenerator\n",
            " |      \n",
            " |      \n",
            " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
            " |                  raise ValueError()\n",
            " |                  yield \"\"\n",
            " |      \n",
            " |      \n",
            " |              def _generate(input: Iterator) -> Iterator[str]:\n",
            " |                  yield from \"foo bar\"\n",
            " |      \n",
            " |      \n",
            " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
            " |                  [RunnableGenerator(_generate)]\n",
            " |                  )\n",
            " |              print(''.join(runnable.stream({}))) #foo bar\n",
            " |      \n",
            " |      Args:\n",
            " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
            " |          exceptions_to_handle: A tuple of exception types to handle.\n",
            " |          exception_key: If string is specified then handled exceptions will be passed\n",
            " |              to fallbacks as part of the input under the specified key. If None,\n",
            " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
            " |              and its fallbacks must accept a dictionary as input.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A new Runnable that will try the original runnable, and then each\n",
            " |          fallback in order, upon failures.\n",
            " |  \n",
            " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
            " |      \n",
            " |      on_start: Called before the runnable starts running, with the Run object.\n",
            " |      on_end: Called after the runnable finishes running, with the Run object.\n",
            " |      on_error: Called if the runnable throws an error, with the Run object.\n",
            " |      \n",
            " |      The Run object contains information about the run, including its id,\n",
            " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
            " |      added to the run.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |          import time\n",
            " |      \n",
            " |          def test_runnable(time_to_sleep : int):\n",
            " |              time.sleep(time_to_sleep)\n",
            " |      \n",
            " |          def fn_start(run_obj : Runnable):\n",
            " |              print(\"start_time:\", run_obj.start_time)\n",
            " |      \n",
            " |          def fn_end(run_obj : Runnable):\n",
            " |              print(\"end_time:\", run_obj.end_time)\n",
            " |      \n",
            " |          RunnableLambda(test_runnable).with_listeners(\n",
            " |              on_start=fn_start,\n",
            " |              on_end=fn_end\n",
            " |              ).invoke(2)\n",
            " |  \n",
            " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
            " |      Create a new Runnable that retries the original runnable on exceptions.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |      \n",
            " |          count = 0\n",
            " |      \n",
            " |      \n",
            " |          def _lambda(x: int) -> None:\n",
            " |              global count\n",
            " |              count = count + 1\n",
            " |              if x == 1:\n",
            " |                  raise ValueError(\"x is 1\")\n",
            " |              else:\n",
            " |                   pass\n",
            " |      \n",
            " |      \n",
            " |          runnable = RunnableLambda(_lambda)\n",
            " |          try:\n",
            " |              runnable.with_retry(\n",
            " |                  stop_after_attempt=2,\n",
            " |                  retry_if_exception_type=(ValueError,),\n",
            " |              ).invoke(1)\n",
            " |          except ValueError:\n",
            " |              pass\n",
            " |      \n",
            " |          assert (count == 2)\n",
            " |      \n",
            " |      \n",
            " |      Args:\n",
            " |          retry_if_exception_type: A tuple of exception types to retry on\n",
            " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
            " |                                   between retries\n",
            " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
            " |      \n",
            " |      Returns:\n",
            " |          A new Runnable that retries the original runnable on exceptions.\n",
            " |  \n",
            " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
            " |  \n",
            " |  config_specs\n",
            " |      List configurable fields for this runnable.\n",
            " |  \n",
            " |  input_schema\n",
            " |      The type of input this runnable accepts specified as a pydantic model.\n",
            " |  \n",
            " |  output_schema\n",
            " |      The type of output this runnable produces specified as a pydantic model.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
            " |  \n",
            " |  name = None\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_2=llm.invoke('What is AI model?', model = 'gpt-4-turbo-preview')\n",
        "print(output_2.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nij74sPWvasQ",
        "outputId": "86ada5ff-da1c-4608-cabe-bcdc2039afe0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An AI model (Artificial Intelligence model) refers to a computational structure designed to perform tasks that typically require human intelligence. These tasks can include recognizing patterns, making decisions, understanding language, and learning from data. AI models are built using algorithms and trained with large datasets to improve their accuracy and performance over time.\n",
            "\n",
            "The development of an AI model involves selecting an appropriate algorithm or combination of algorithms, feeding them data, and refining the model based on its performance. The model learns from the training data by identifying patterns and making inferences, a process known as machine learning (ML), which is a subset of AI.\n",
            "\n",
            "There are various types of AI models, each suited for different tasks. Some of the common types include:\n",
            "\n",
            "1. **Machine Learning Models**: These are the most common types of AI models, where the system learns from data to improve its performance on a specific task.\n",
            "  \n",
            "2. **Deep Learning Models**: A subset of machine learning that uses neural networks with many layers (hence \"deep\"). These models are particularly good at recognizing patterns in images, sound, text, and other complex datasets.\n",
            "  \n",
            "3. **Neural Networks**: Inspired by the human brain, these models are composed of nodes (neurons) and edges (synapses) that can learn to perform tasks by considering examples, rather than being programmed with specific rules.\n",
            "  \n",
            "4. **Natural Language Processing (NLP) Models**: These models are designed to understand, interpret, and generate human language in a way that is valuable.\n",
            "  \n",
            "5. **Reinforcement Learning Models**: These models learn by interacting with their environment, using feedback from their own actions and experiences to make decisions.\n",
            "\n",
            "AI models are applied in various domains, including but not limited to, autonomous vehicles, medical diagnosis, natural language processing, image recognition, and financial modeling. The creation and refinement of AI models are central to the field of artificial intelligence, driving innovations and improving the capabilities of AI systems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage (content = 'You are an expert crypto trader and you answer the question percisely and meticolously.'),\n",
        "    HumanMessage (content = 'What is the future of the cypto market?')\n",
        "    ]\n",
        "\n",
        "output_3 = llm.invoke (messages)\n",
        "print(output_3.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75BfUkCXw0XA",
        "outputId": "b9e66052-103e-4376-bee6-eee6a11c403f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a crypto trader, it is important to note that predicting the future of the crypto market with absolute certainty is challenging due to its volatile nature. However, there are several trends and factors that can provide insights into the potential future of the market:\n",
            "\n",
            "1. **Adoption and Integration**: Increased adoption of cryptocurrencies by individuals, businesses, and institutions can lead to a more widespread acceptance of digital assets. This could potentially drive up demand and increase the overall market cap of cryptocurrencies.\n",
            "\n",
            "2. **Regulatory Developments**: Regulatory actions by governments and regulatory bodies can have a significant impact on the crypto market. Clear regulations can provide more confidence to investors and encourage further adoption, while stringent regulations can create uncertainty and hinder growth.\n",
            "\n",
            "3. **Technological Advancements**: Continued advancements in blockchain technology, such as scalability solutions and interoperability protocols, can improve the efficiency and usability of cryptocurrencies. This can attract more users and investors to the market.\n",
            "\n",
            "4. **Market Sentiment**: Market sentiment plays a crucial role in determining the direction of the crypto market. Factors such as macroeconomic trends, geopolitical events, and investor sentiment can influence the prices of cryptocurrencies.\n",
            "\n",
            "5. **Competition and Innovation**: The crypto market is highly competitive, with new projects and technologies constantly emerging. Projects that offer unique value propositions and innovative solutions are likely to attract attention and investment.\n",
            "\n",
            "6. **Bitcoin Dominance**: Bitcoin's dominance in the market can also impact the overall market trends. Changes in Bitcoin's price and market share often have a ripple effect on other cryptocurrencies.\n",
            "\n",
            "In conclusion, while it is impossible to predict the future of the crypto market with certainty, paying attention to these trends and factors can help traders make informed decisions and navigate the volatile market effectively. It is essential to conduct thorough research, stay updated on market developments, and manage risks prudently to succeed in the crypto trading landscape.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Memory Cache"
      ],
      "metadata": {
        "id": "Dn2OODLW4GbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model= 'gpt-3.5-turbo-instruct')"
      ],
      "metadata": {
        "id": "4JqxyEM84OGO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# measure the execution time\n",
        "%%time\n",
        "from langchain.cache import InMemoryCache\n",
        "set_llm_cache(InMemoryCache())\n",
        "prompt = 'what is a robot?'\n",
        "llm.invoke (prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "l-flMBYW4-0t",
        "outputId": "3aa7375c-d62f-4f8c-e2b3-753b84fa3408"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 382 ms, sys: 45.3 ms, total: 428 ms\n",
            "Wall time: 2.24 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nA robot is a mechanical or electronic device that is designed to perform tasks automatically, either autonomously or under the control of a human operator. Robots can be programmed to perform a wide variety of tasks, including manufacturing, transportation, exploration, and household chores. They typically have sensors, processors, and actuators that allow them to perceive and interact with their environment. Some robots are designed to look and move like humans, while others have more simplistic designs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "llm.invoke (prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "NEp5ImIx6o7F",
        "outputId": "030399e9-f723-4f2f-f779-60eecb259968"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 710 µs, sys: 0 ns, total: 710 µs\n",
            "Wall time: 721 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nA robot is a mechanical or electronic device that is designed to perform tasks automatically, either autonomously or under the control of a human operator. Robots can be programmed to perform a wide variety of tasks, including manufacturing, transportation, exploration, and household chores. They typically have sensors, processors, and actuators that allow them to perceive and interact with their environment. Some robots are designed to look and move like humans, while others have more simplistic designs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQLite chching"
      ],
      "metadata": {
        "id": "3um07Zpt66Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "set_llm_cache (SQLiteCache(database_path= '.langchain.db' ))\n",
        "\n",
        "# longer time response\n",
        "llm.invoke (prompt)\n",
        "\n",
        "# shorter time response\n",
        "\n",
        "llm.invoke (prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9sRWLmvn640k",
        "outputId": "7b2f793c-af4f-423c-b455-7d1b7c010ef6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nA robot is a machine designed to perform tasks automatically, typically with some level of intelligence and autonomy. Robots can be found in various industries and fields, such as manufacturing, medicine, space exploration, and entertainment. They can be controlled by humans or operate autonomously through programming and sensors. Some robots are designed to look and act like humans, while others have a more mechanical appearance. Overall, robots are designed to increase efficiency, accuracy, and productivity in various tasks and industries.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM Streaming"
      ],
      "metadata": {
        "id": "ddmyxkgEQqnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "prompt = 'Write a lovely song'\n",
        "print(llm.invoke(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtYaM6tXQs2t",
        "outputId": "2d031ded-d05e-4b17-fd61-033b92b3cce9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Verse 1:\n",
            "Looking into your eyes\n",
            "I see the stars align\n",
            "Your smile lights up the room\n",
            "And I can't help but swoon\n",
            "\n",
            "Chorus:\n",
            "You're my everything, my love\n",
            "Sent to me from above\n",
            "With you, every day is bright\n",
            "You make my world just right\n",
            "\n",
            "Verse 2:\n",
            "Your touch is like a flame\n",
            "Melting away my pain\n",
            "I feel so alive with you\n",
            "There's nothing that I can't do\n",
            "\n",
            "Chorus:\n",
            "You're my everything, my love\n",
            "Sent to me from above\n",
            "With you, every day is bright\n",
            "You make my world just right\n",
            "\n",
            "Bridge:\n",
            "I never believed in love at first sight\n",
            "But then you came along and changed my mind\n",
            "You showed me what true love could be\n",
            "And now I know you're the one for me\n",
            "\n",
            "Chorus:\n",
            "You're my everything, my love\n",
            "Sent to me from above\n",
            "With you, every day is bright\n",
            "You make my world just right\n",
            "\n",
            "Outro:\n",
            "With you by my side\n",
            "I know I'll never hide\n",
            "My love for you will always stay\n",
            "Forever and always, my heart will say.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream(prompt):\n",
        "  print(chunk, end = '', flush = True)"
      ],
      "metadata": {
        "id": "8GA5FIMZTX5V",
        "outputId": "07de0148-2053-4b90-d118-86ec72dd15bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verse 1:\n",
            "The sun is shining, the birds are singing\n",
            "As I walk down this path, my heart is ringing\n",
            "With a melody that's never been heard\n",
            "A love song that's meant for you, my dear\n",
            "\n",
            "Chorus:\n",
            "You are my everything, my shining star\n",
            "With you by my side, I'll never be far\n",
            "Your love is like a symphony\n",
            "A beautiful song that's meant for me\n",
            "\n",
            "Verse 2:\n",
            "Through all the storms and darkest nights\n",
            "You've been my shelter, my guiding light\n",
            "With every beat of my heart, I feel your love\n",
            "A gift from above, a blessing from up above\n",
            "\n",
            "Chorus:\n",
            "You are my everything, my shining star\n",
            "With you by my side, I'll never be far\n",
            "Your love is like a symphony\n",
            "A beautiful song that's meant for me\n",
            "\n",
            "Bridge:\n",
            "With every note and every rhyme\n",
            "I'll show you, my love, time after time\n",
            "I'll sing this song, forever and always\n",
            "For you, my dear, in every single way\n",
            "\n",
            "Chorus:\n",
            "You are my everything, my shining star\n",
            "With you by my side, I'll never be far\n",
            "Your love is like a symphony\n",
            "A beautiful song that's meant for"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PromptTemplate"
      ],
      "metadata": {
        "id": "5FaTJjBBVyXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "template = ''' You are an expert in Robotics software developing. Explain the {Robot} application in {industry} industry.'''\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "prompt = prompt_template.format(Robot = 'delta', industry = 'healthcare')\n",
        "prompt"
      ],
      "metadata": {
        "id": "5-Zx3ouvV3oI",
        "outputId": "d9cd6189-6fa5-4df8-dd89-2d221fb73251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' You are an expert in Robotics software developing. Explain the delta application in healthcare industry.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature = 0.4)\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "_RnBsNp3YFl8",
        "outputId": "18f4982d-6196-45b1-8cc3-c394c805b1d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the healthcare industry, the delta application in robotics software development refers to the use of delta robots in various medical procedures and tasks. Delta robots are specialized robots that are known for their speed, precision, and flexibility, making them ideal for a wide range of applications in healthcare.\n",
            "\n",
            "One common use of delta robots in healthcare is in surgical procedures. These robots can be used to assist surgeons in performing minimally invasive surgeries with greater precision and accuracy. Delta robots can also be used in telemedicine applications, allowing doctors to remotely perform surgeries or procedures on patients in different locations.\n",
            "\n",
            "Delta robots can also be used in the manufacturing of medical devices and equipment. These robots can be programmed to assemble and package medical devices with high levels of precision and efficiency, ensuring that they meet the strict quality standards required in the healthcare industry.\n",
            "\n",
            "Overall, the delta application in healthcare industry offers numerous benefits, including improved patient outcomes, increased efficiency, and reduced costs. As an expert in robotics software development, you can play a crucial role in developing and implementing these applications to help advance the field of healthcare.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatPromptTemplate"
      ],
      "metadata": {
        "id": "NL90TG7QZkbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content = 'You respond only in JSON format'),\n",
        "    HumanMessagePromptTemplate.from_template ('The most famous four {robot} in {field} field.')\n",
        "])\n",
        "\n",
        "message = chat_template.format_messages(robot = 'drone', field ='healthcare')\n",
        "print (message)"
      ],
      "metadata": {
        "id": "WHkraWfjZnRr",
        "outputId": "651b6386-2eb0-430d-c68f-a63f7c23600c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You respond only in JSON format'), HumanMessage(content='The most famous four drone in healthcare field.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output= llm.invoke(message)\n",
        "print(output.content)"
      ],
      "metadata": {
        "id": "_0dRo7iCdA3K",
        "outputId": "c483616f-cfad-439a-d7c2-e44a03aa4568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"drones\": [\n",
            "    {\n",
            "      \"name\": \"Zipline\",\n",
            "      \"application\": \"Medical supplies delivery\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Matternet\",\n",
            "      \"application\": \"Medical samples and supplies delivery\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Flirtey\",\n",
            "      \"application\": \"Medical supplies delivery\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Vayu\",\n",
            "      \"application\": \"Medical supplies delivery\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Chain"
      ],
      "metadata": {
        "id": "3eu79-FeiesB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import LLMChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "template = ''' You are an expert in Robotics software developing. Explain the {Robot} application in {industry} industry.'''\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = LLMChain(\n",
        "\n",
        "                 llm = llm,\n",
        "                 prompt = prompt_template,\n",
        "                 verbose = True\n",
        "\n",
        ")\n",
        "\n",
        "output = chain.invoke ({'Robot':'drone', 'industry':'healthcare'})\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ajQF6fX0igt2",
        "outputId": "8b4dfd46-ae33-41c3-9a53-348f5a4dbf39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m You are an expert in Robotics software developing. Explain the drone application in healthcare industry.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'Robot': 'drone', 'industry': 'healthcare', 'text': 'Drones have a wide range of applications in the healthcare industry, from delivering medical supplies to remote areas to assisting in medical emergencies. As a robotics software developer, you can play a crucial role in developing and implementing drone technology in healthcare.\\n\\nOne of the primary uses of drones in healthcare is for the delivery of medical supplies and equipment to remote or hard-to-reach areas. This can include transporting medications, vaccines, blood samples, and even organs for transplant. Drones can significantly reduce delivery times and costs compared to traditional methods, particularly in areas with limited infrastructure or during emergencies.\\n\\nDrones can also be used for emergency medical services, such as delivering defibrillators or other life-saving equipment to patients in need. In cases of natural disasters or mass casualties, drones equipped with medical supplies can provide rapid assistance to those in need.\\n\\nIn addition, drones can be used for monitoring and surveillance in healthcare settings, such as tracking the spread of infectious diseases or monitoring patients remotely. They can also be used for transporting medical samples or conducting medical imaging in hard-to-reach or dangerous locations.\\n\\nAs a robotics software developer, you can help design and program drones to safely and efficiently carry out these tasks. This may involve developing algorithms for route optimization, obstacle avoidance, and payload management, as well as integrating sensors and other technology to ensure safe and accurate delivery of medical supplies.\\n\\nOverall, drones have the potential to revolutionize healthcare delivery by providing faster, more efficient, and cost-effective solutions for transporting medical supplies and providing emergency medical services. By leveraging your expertise in robotics software development, you can help bring this technology to the forefront of the healthcare industry.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = 'What is the best food of {country}? Name five most to see places in it. use bulletpoints format.'\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = LLMChain(\n",
        "\n",
        "                 llm = llm,\n",
        "                 prompt = prompt_template,\n",
        "                 verbose = True\n",
        "\n",
        ")\n",
        "\n",
        "country = input('country')\n",
        "output = chain.invoke(country)\n",
        "print(output['text'])"
      ],
      "metadata": {
        "id": "c46SLbmhlSwn",
        "outputId": "8392672e-d407-4de9-cb7c-0adf029db50b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "countryGermany\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWhat is the best food of Germany? Name five most to see places in it. use bulletpoints format.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "- The best food of Germany includes:\n",
            "\n",
            "1. Bratwurst (grilled sausage)\n",
            "2. Schnitzel (breaded and fried meat)\n",
            "3. Sauerkraut (fermented cabbage)\n",
            "4. Pretzels (baked bread with a twisted shape)\n",
            "5. Black Forest cake (chocolate sponge cake with cherries and cream)\n",
            "\n",
            "- Five most to see places in Germany:\n",
            "\n",
            "1. Neuschwanstein Castle in Bavaria\n",
            "2. Brandenburg Gate in Berlin\n",
            "3. Cologne Cathedral in Cologne\n",
            "4. The Black Forest region in southwestern Germany\n",
            "5. The Romantic Road, a scenic route through Bavaria and Baden-Württemberg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain Agents in Action: Python REPL"
      ],
      "metadata": {
        "id": "t8wpUNar5ztD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_experimental"
      ],
      "metadata": {
        "id": "ssHLvV4P5o8z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "# REPL is an acronym for Read, Evaluate, Print, and Loop.\n",
        "# Developers use REPL Python to communicate with the Python Interpreter.\n",
        "# In contrast to running a Python file, you can input commands in the REPL and see the results displayed immediately.\n",
        "\n",
        "python_repl = PythonREPL()\n",
        "python_repl.run('print([n for n in range (1,100) if n%13==0])')"
      ],
      "metadata": {
        "id": "TdLdivy_7y1d",
        "outputId": "9a930532-38b7-4573-b3ec-b11dbd54f74a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[13, 26, 39, 52, 65, 78, 91]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model = 'gpt-4-turbo-preview', temperature=0)\n",
        "agent_executer = create_python_agent(\n",
        "    llm = llm,\n",
        "    tool = PythonREPLTool(),\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "\n",
        "agent_executer.invoke('calculate 5.1**7.1')\n",
        "\n",
        "print ('------------------')\n",
        "\n",
        "agent_executer.invoke('calculate the square root of factorial of 12 and display it with 5 decimal points')"
      ],
      "metadata": {
        "id": "kIwv1NtzKVmt",
        "outputId": "154fc7c5-3e97-4c55-8b71-d978363472f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo calculate \\(5.1^{7.1}\\), I will use Python's exponentiation operator `**`.\n",
            "Action: Python_REPL\n",
            "Action Input: print(5.1**7.1)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m105620.50759235727\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 105620.50759235727\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "------------------\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mFirst, I need to calculate the factorial of 12. Then, I will find the square root of that result. Finally, I will format the output to display it with 5 decimal points.\n",
            "Action: Python_REPL\n",
            "Action Input: import math\n",
            "print(f\"{math.sqrt(math.factorial(12)):.5f}\")\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m21886.10518\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 21886.10518\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'calculate the square root of factorial of 12 and display it with 5 decimal points',\n",
              " 'output': '21886.10518'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain Tools: DuckDuckGo and Wikipedia"
      ],
      "metadata": {
        "id": "8CSkXYMZJJyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1_AwW-eJJIb",
        "outputId": "0e1082c8-2813-4d91-f822-a9563d400d50"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def search_duckduckgo(query):\n",
        "    url = f\"https://duckduckgo.com/html/?q={query}\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    results = []\n",
        "    for result in soup.find_all('a', class_='result__a'):\n",
        "        title = result.get_text()\n",
        "        link = result['href']\n",
        "        results.append({'title': title, 'link': link})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example use:\n",
        "output = search_duckduckgo('What is deep learning?')\n",
        "for item in output:\n",
        "    print(f\"Title: {item['title']}\\nLink: {item['link']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6uZV1X4JU9b",
        "outputId": "914bb96e-8c0b-4aa9-a522-1f3b83d1721c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Deep learning - Wikipedia\n",
            "Link: https://en.wikipedia.org/wiki/Deep_learning\n",
            "\n",
            "Title: What is Deep Learning? | IBM\n",
            "Link: https://www.ibm.com/topics/deep-learning\n",
            "\n",
            "Title: What Is Deep Learning? Definition, Examples, and Careers\n",
            "Link: https://www.coursera.org/articles/what-is-deep-learning\n",
            "\n",
            "Title: What Is Deep Learning and How Does It Work? | Built In\n",
            "Link: https://builtin.com/machine-learning/deep-learning\n",
            "\n",
            "Title: What Is Deep Learning? Definition, Examples, and Careers\n",
            "Link: https://www.coursera.org/ca/articles/what-is-deep-learning\n",
            "\n",
            "Title: What is Deep Learning? A Tutorial for Beginners | DataCamp\n",
            "Link: https://www.datacamp.com/tutorial/tutorial-deep-learning-tutorial\n",
            "\n",
            "Title: What is Deep Learning? Applications & Examples | Google Cloud\n",
            "Link: https://cloud.google.com/discover/what-is-deep-learning\n",
            "\n",
            "Title: What is deep learning? | Cloudflare\n",
            "Link: https://www.cloudflare.com/learning/ai/what-is-deep-learning/\n",
            "\n",
            "Title: Deep learning | Nature\n",
            "Link: https://www.nature.com/articles/nature14539\n",
            "\n",
            "Title: What is deep learning? Everything you need to know | ZDNET\n",
            "Link: https://www.zdnet.com/article/what-is-deep-learning-everything-you-need-to-know/\n",
            "\n",
            "Title: What is deep learning? | McKinsey\n",
            "Link: https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-deep-learning\n",
            "\n",
            "Title: What Is Deep Learning? | A Beginner's Guide - Scribbr\n",
            "Link: https://www.scribbr.com/ai-tools/deep-learning/\n",
            "\n",
            "Title: What is deep learning and how does it work? - TechTarget\n",
            "Link: https://www.techtarget.com/searchenterpriseai/definition/deep-learning-deep-neural-network\n",
            "\n",
            "Title: Introduction to Deep Learning - GeeksforGeeks\n",
            "Link: https://www.geeksforgeeks.org/introduction-deep-learning/\n",
            "\n",
            "Title: What is Deep Learning? - Deep Learning Explained - AWS\n",
            "Link: https://aws.amazon.com/what-is/deep-learning/\n",
            "\n",
            "Title: What is Deep Learning? - MachineLearningMastery.com\n",
            "Link: https://machinelearningmastery.com/what-is-deep-learning/\n",
            "\n",
            "Title: What is Deep Learning? | Oracle\n",
            "Link: https://www.oracle.com/artificial-intelligence/machine-learning/what-is-deep-learning/\n",
            "\n",
            "Title: What Is Deep Learning? | Microsoft Azure\n",
            "Link: https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-deep-learning/\n",
            "\n",
            "Title: What Is Deep Learning? Definition, Examples, and Careers\n",
            "Link: https://www.coursera.org/in/articles/what-is-deep-learning\n",
            "\n",
            "Title: What Is Deep Learning? | PCMag\n",
            "Link: https://www.pcmag.com/news/what-is-deep-learning\n",
            "\n",
            "Title: Deep Learning - MATLAB & Simulink - MathWorks\n",
            "Link: https://www.mathworks.com/discovery/deep-learning.html\n",
            "\n",
            "Title: Discover Deep Learning: AI's Game-Changing Technology! - Simplilearn\n",
            "Link: https://www.simplilearn.com/tutorials/deep-learning-tutorial/what-is-deep-learning\n",
            "\n",
            "Title: What is machine learning? | McKinsey\n",
            "Link: https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-machine-learning\n",
            "\n",
            "Title: Deep Learning Illustrated, Part 3: Convolutional Neural Networks\n",
            "Link: https://towardsdatascience.com/deep-learning-illustrated-part-3-convolutional-neural-networks-96b900b0b9e0\n",
            "\n",
            "Title: FPGA vs. GPU: Which is better for deep learning? - IBM Blog\n",
            "Link: https://www.ibm.com/blog/fpga-vs-gpu/\n",
            "\n",
            "Title: Deep Learning vs. Machine Learning: A Beginner's Guide\n",
            "Link: https://www.coursera.org/articles/ai-vs-deep-learning-vs-machine-learning-beginners-guide\n",
            "\n",
            "Title: Nick Bostrom's 'Deep Utopia' On Our AI Future: Can We ... - Forbes\n",
            "Link: https://www.forbes.com/sites/calumchace/2024/05/09/can-we-have-meaning-as-well-as-fun-nick-bostroms-deep-utopia/\n",
            "\n",
            "Title: Deep Learning Makes a Splash in Groundwater Modeling\n",
            "Link: https://engineering.jhu.edu/case/news/deep-learning-makes-a-splash-in-groundwater-modeling/\n",
            "\n",
            "Title: AI Deep Learning Improves Brain-Computer Interface Performance\n",
            "Link: https://www.psychologytoday.com/us/blog/the-future-brain/202405/ai-deep-learning-improves-brain-computer-interface-performance\n",
            "\n",
            "Title: What is Nvidia's DLSS and can it improve your gaming performance ...\n",
            "Link: https://reviewed.usatoday.com/gaming/features/what-is-dlss-nvidia\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PnisT9rgJtPz",
        "outputId": "126c9110-4003-47d9-aa8e-f08eadf7831b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'duckduckgo_search'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_gevJf8jJv77",
        "outputId": "d1d2b2f2-77c4-4e35-d1c1-4a23532d442e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "\n",
        "search= DuckDuckGoSearchResults()\n",
        "output= search.run('What is deep learning?')\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "AMb9KTqoJyGT",
        "outputId": "43d829fe-804b-450d-bdcd-258d1cc28d14"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RatelimitException",
          "evalue": "https://links.duckduckgo.com/d.js?q=What+is+deep+learning%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-265844750037496071511915622490926274375&ex=-1 202 Ratelimit",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRatelimitException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-785d78501c88>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDuckDuckGoSearchResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'What is deep learning?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/tools.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/tools.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_args_and_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             observation = (\n\u001b[0;32m--> 409\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m    410\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtool_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/tools/ddg_search/tool.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m     62\u001b[0m     ) -> str:\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m\"\"\"Use the tool.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mres_strs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{k}: {v}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"[{rs}]\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres_strs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/duckduckgo_search.py\u001b[0m in \u001b[0;36mresults\u001b[0;34m(self, query, max_results, source)\u001b[0m\n\u001b[1;32m    109\u001b[0m             results = [\n\u001b[1;32m    110\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"snippet\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"link\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"href\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ddgs_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             ]\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"news\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/duckduckgo_search.py\u001b[0m in \u001b[0;36m_ddgs_text\u001b[0;34m(self, query, max_results)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDDGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mddgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             ddgs_gen = ddgs.text(\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mregion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/duckduckgo_search/duckduckgo_search.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_async_in_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/duckduckgo_search/duckduckgo_search.py\u001b[0m in \u001b[0;36m_run_async_in_thread\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;34m\"\"\"Runs an async coroutine in a separate thread.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_coroutine_threadsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/duckduckgo_search/duckduckgo_search_async.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"api\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafesearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimelimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"html\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafesearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimelimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/duckduckgo_search/duckduckgo_search_async.py\u001b[0m in \u001b[0;36m_text_api\u001b[0;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/duckduckgo_search/duckduckgo_search_async.py\u001b[0m in \u001b[0;36m_text_api\u001b[0;34m(self, keywords, region, safesearch, timelimit, max_results)\u001b[0m\n\u001b[1;32m    251\u001b[0m             )\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/duckduckgo_search/duckduckgo_search_async.py\u001b[0m in \u001b[0;36m_text_api_page\u001b[0;34m(s, page)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mpriority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{s}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mresp_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aget_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https://links.duckduckgo.com/d.js\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mpage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_text_extract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/duckduckgo_search/duckduckgo_search_async.py\u001b[0m in \u001b[0;36m_aget_url\u001b[0;34m(self, method, url, data, params)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m202\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m301\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRatelimitException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{resp.url} {resp.status_code} Ratelimit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mDuckDuckGoSearchException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{resp.url} return None. {params=} {data=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRatelimitException\u001b[0m: https://links.duckduckgo.com/d.js?q=What+is+deep+learning%3F&kl=wt-wt&l=wt-wt&p=&s=0&df=y&vqd=4-265844750037496071511915622490926274375&ex=-1 202 Ratelimit"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "\n",
        "wrapper = DuckDuckGoSearchAPIWrapper(region='de-de', max_results=3, safesearch='moderate')\n",
        "search=DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
        "output=search.run('Berlin')\n",
        "print(output)"
      ],
      "metadata": {
        "id": "rk8MeLKCKJcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-V7gmDqLK16b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'snippet:(.*?),title:(.*?),link:(.*?)\\]'\n",
        "matches = re.findall(pattern, output, re.DOTALL)\n",
        "\n",
        "for snippet, title, link in matches:\n",
        "  print(f'Snippet:{snippet}\\nTitle:{title}\\nLink:{link}\\n')\n",
        "  print('-'*50)\n"
      ],
      "metadata": {
        "id": "srkq0LGqLEzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9B0BfnqL_u-",
        "outputId": "24a812d9-d898-46b1-808a-57f9d45a2d0c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "api_wrapper= WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=500)\n",
        "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "wiki.invoke({'query':'llama-index'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "EGV1avyeMEXc",
        "outputId": "cf40ece0-ecc8-42d8-ae23-0366f2ed5890"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: Vector database\\nSummary: A vector database management system or simply vector database or vector store is a database that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more Approximate Nearest Neighbor (ANN) algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\\nVectors are mathematical representations of data in a high-dimensional space. In this spac'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki.invoke('Google Gemini')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "-0V2ww-6NE-6",
        "outputId": "316a3f56-0ee7-4480-cd6d-5c1d11b6dd39"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Page: Gemini (chatbot)\\nSummary: Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name and developed as a direct response to the meteoric rise of OpenAI's ChatGPT, it was launched in a limited capacity in March 2023 before expanding to other countries in May. It was previously based on PaLM, and initially the LaMDA family of large language models. \\nLaMDA had been developed and announced in 2021\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating ReAct Agent"
      ],
      "metadata": {
        "id": "xLNoD2YIaHiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchainhub"
      ],
      "metadata": {
        "id": "YcpFe-4zaG28"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(\"/content/drive/MyDrive/Udemy/LangChain/p-1/.env\", override=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRXXsY7Baa11",
        "outputId": "c9815ad4-0d6f-4d10-f318-dee7e952c0bb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
        "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0)\n",
        "\n",
        "template= '''\n",
        "Answer the following questions as best you can.\n",
        "Question: {q}\n",
        "'''\n",
        "\n",
        "\n",
        "# template= '''\n",
        "# Answer the following questions in Spanish as best you can.\n",
        "# Question: {q}\n",
        "# '''\n",
        "prompt_template = PromptTemplate.from_template(template)\n",
        "prompt = hub.pull('hwchase17/react')\n",
        "\n",
        "# print(type(prompt))\n",
        "# print(prompt.input_variables)\n",
        "# print(prompt.template)\n",
        "\n",
        "# 1. Python REPL tool (for executing python code)\n",
        "\n",
        "python_repl=PythonREPLTool()\n",
        "python_repl_tool = Tool(\n",
        "    name='Python REPL',\n",
        "    func = python_repl.run,\n",
        "    description= 'To execute Python code.'\n",
        "\n",
        ")\n",
        "\n",
        "# 2. Wikipedia tool (for searching wikipedia)\n",
        "api_wrapper = WikipediaAPIWrapper()\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "wikipedia_tool= Tool(\n",
        "    name='Wikipedia',\n",
        "    func = wikipedia.run,\n",
        "    description= 'Search in Wikipedia.'\n",
        ")\n",
        "\n",
        "# 3. DuckDuckGo search tool (for geberal web search)\n",
        "search = DuckDuckGoSearchRun()\n",
        "duckduckgo_tool= Tool(\n",
        "    name = 'DuckDuckGo Search',\n",
        "    func = search.run,\n",
        "    description= 'Search in Internet'\n",
        ")\n",
        "\n",
        "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
        "\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executer= AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors= True,\n",
        "    max_iterations=10\n",
        ")"
      ],
      "metadata": {
        "id": "vBfIcg5sag0c"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = 'Generate the first 20 numbers in fibonacci series.'\n",
        "question = 'Who is the president of USA?'\n",
        "output = agent_executer.invoke({\n",
        "    'input': prompt_template.format(q=question)\n",
        "})"
      ],
      "metadata": {
        "id": "7CuOoIDXiqog",
        "outputId": "7cf216bb-8957-496e-e2e2-bdf9093bbe38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI know that as of my last update in 2023, Joe Biden is the President of the United States. However, to confirm that this information is still current, I should check a reliable source.\n",
            "\n",
            "Action: Wikipedia\n",
            "Action Input: President of the United States\u001b[0m\u001b[33;1m\u001b[1;3mPage: President of the United States\n",
            "Summary: The president of the United States (POTUS) is the head of state and head of government of the United States of America. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\n",
            "The power of the presidency has grown substantially since the first president, George Washington, took office in 1789. While presidential power has ebbed and flowed over time, the presidency has played an increasingly significant role in American political life since the beginning of the 20th century, carrying over into the 21st century with notable expansions during the presidencies of Franklin D. Roosevelt and George W. Bush. In modern times, the president is one of the world's most powerful political figures and the leader of the world's only remaining superpower. As the leader of the nation with the largest economy by nominal GDP, the president possesses significant domestic and international hard and soft power. For much of the 20th century, especially during the Cold War, the U.S. president was often called \"the leader of the free world\".\n",
            "Article II of the Constitution establishes the executive branch of the federal government and vests executive power in the president. The power includes the execution and enforcement of federal law and the responsibility to appoint federal executive, diplomatic, regulatory, and judicial officers.  Based on constitutional provisions empowering the president to appoint and receive ambassadors and conclude treaties with foreign powers, and on subsequent laws enacted by Congress, the modern presidency has primary responsibility for conducting U.S. foreign policy. The role includes responsibility for directing the world's most expensive military, which has the second-largest nuclear arsenal.\n",
            "The president also plays a leading role in federal legislation and domestic policymaking. As part of the system of separation of powers, Article I, Section 7 of the Constitution gives the president the power to sign or veto federal legislation. Since modern presidents are typically viewed as leaders of their political parties, major policymaking is significantly shaped by the outcome of presidential elections, with presidents taking an active role in promoting their policy priorities to members of Congress who are often electorally dependent on the president. In recent decades, presidents have also made increasing use of executive orders, agency regulations, and judicial appointments to shape domestic policy.\n",
            "The president is elected indirectly through the Electoral College to a four-year term, along with the vice president. Under the Twenty-second Amendment, ratified in 1951, no person who has been elected to two presidential terms may be elected to a third. In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation. In all, 45 individuals have served 46 presidencies spanning 58 four-year terms. Joe Biden is the 46th and current president of the United States, having assumed office on January 20, 2021.\n",
            "\n",
            "Page: List of presidents of the United States\n",
            "Summary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of individuals who have served as president.\n",
            "The presidency of William Henry Harrison, who died 31 days after taking office in 1841, \u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: Joe Biden is the current president of the United States, having assumed office on January 20, 2021.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['output']"
      ],
      "metadata": {
        "id": "10TOZscTjx0Q",
        "outputId": "c16d72ec-cc8b-472c-ebe3-888b36b93383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Joe Biden is the current president of the United States, having assumed office on January 20, 2021.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ]
}